{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2bc687",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effad654-436d-400e-97cc-8bd36141370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import subprocess\n",
    "import piexif\n",
    "from datetime import datetime\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560725a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# MPS seems to crash every now and then\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5ce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "base_path='/Users/peter/playground/parkbridge'\n",
    "folder_path = f'{base_path}/images'\n",
    "warped_images_folder = f'{base_path}/auto_warped'\n",
    "base_image = 'IMG_20240820_074416.jpg'\n",
    "\n",
    "#1\n",
    "matched_segments_bbox_expand = 0.05\n",
    "min_segment_surface = 0.0015\n",
    "max_segment_surface = 0.15\n",
    "match_distance_penalty_factor=0.1\n",
    "match_distance_threshold=0.015\n",
    "\n",
    "#2\n",
    "# matched_segments_bbox_expand = 0.10\n",
    "# min_segment_surface = 0.0010\n",
    "# max_segment_surface = 0.20\n",
    "# match_distance_penalty_factor=0.1\n",
    "# match_distance_threshold=0.010\n",
    "\n",
    "#3\n",
    "# matched_segments_bbox_expand = 0.10\n",
    "# min_segment_surface = 0.0010\n",
    "# max_segment_surface = 0.20\n",
    "# match_distance_penalty_factor=0.15\n",
    "# match_distance_threshold=0.015\n",
    "\n",
    "\n",
    "stop_motion_result_path=f'{base_path}/auto_parkbridge.mp4'\n",
    "\n",
    "\n",
    "#Gent\n",
    "# folder_path = f'{base_path}/gent'\n",
    "# warped_images_folder = f'{base_path}/gent_warped'\n",
    "# base_image = '20241014_085000.jpg'\n",
    "# stop_motion_result_path=f'{base_path}/gent.mp4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b6e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask \n",
    "        if borders:\n",
    "            import cv2\n",
    "            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "            # Try to smooth contours\n",
    "            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "    ax.imshow(img)\n",
    "\n",
    "def downscale_image_by_percentage(image, scale_percent):\n",
    "    \"\"\"\n",
    "    Downscale the image by a percentage while maintaining the aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image or numpy.ndarray): The input image to downscale.\n",
    "        scale_percent (float): The percentage to scale the image by (e.g., 50 for 50% of the original size).\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The downscaled image.\n",
    "    \"\"\"\n",
    "    # If the image is in NumPy format, convert it back to a PIL Image\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "    # Calculate the new size based on the scale percentage\n",
    "    width, height = image.size\n",
    "    new_width = int(width * scale_percent / 100)\n",
    "    new_height = int(height * scale_percent / 100)\n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Resize the image to the new size\n",
    "    downscaled_image = image.resize(new_size,  Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return downscaled_image\n",
    "\n",
    "def load_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = downscale_image_by_percentage(image, scale_percent=100)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    return image\n",
    "\n",
    "def plt_image(image):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def expand_bounding_boxes_in_masks(image, masks, expand_percent=0.0):\n",
    "    \"\"\"\n",
    "    Expand the bounding boxes of all masks by a given percentage, while ensuring they stay within image boundaries,\n",
    "    and update the masks in place with the expanded bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        masks (list): List of masks generated by SAM, each with a 'bbox' key.\n",
    "        image (numpy.ndarray): The image from which the bounding box is extracted (to get dimensions).\n",
    "        expand_percent (float): The percentage by which to expand the bounding boxes (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        list: The updated list of masks with modified 'bbox' values.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the image\n",
    "    image_height, image_width = image.shape[:2]  # Height and width from the image dimensions\n",
    "    \n",
    "    # Iterate over all masks and expand their bounding boxes\n",
    "    for mask in masks:\n",
    "        x_min, y_min, width, height = mask['bbox']\n",
    "        \n",
    "        # Calculate the amount to expand in each direction\n",
    "        expand_w = width * expand_percent\n",
    "        expand_h = height * expand_percent\n",
    "        \n",
    "        # Calculate the new bounding box\n",
    "        new_x_min = max(0, x_min - expand_w / 2)  # Ensure it doesn't go below 0\n",
    "        new_y_min = max(0, y_min - expand_h / 2)  # Ensure it doesn't go below 0\n",
    "        \n",
    "        new_x_max = min(image_width, x_min + width + expand_w / 2)  # Ensure it doesn't exceed image width\n",
    "        new_y_max = min(image_height, y_min + height + expand_h / 2)  # Ensure it doesn't exceed image height\n",
    "        \n",
    "        # Calculate new width and height based on the expanded coordinates\n",
    "        new_width = new_x_max - new_x_min\n",
    "        new_height = new_y_max - new_y_min\n",
    "        \n",
    "        # Update the 'bbox' in the mask with the expanded bounding box\n",
    "        mask['bbox'] = (new_x_min, new_y_min, new_width, new_height)\n",
    "    \n",
    "    # Return the updated list of masks\n",
    "    return masks\n",
    "\n",
    "def find_matching_segment_with_distance_penalty(template, target_image, template_bbox, penalty_factor, distance_threshold):\n",
    "    \"\"\"\n",
    "    Use cv2.matchTemplate to find the location of the segment in the target image,\n",
    "    and penalize the match score based on how far the match is from the original template's bounding box.\n",
    "    \n",
    "    Args:\n",
    "        template (numpy.ndarray): The extracted segment (template) from the first image.\n",
    "        target_image (numpy.ndarray): The target image in which to search for the template.\n",
    "        template_bbox (tuple): The bounding box of the template in the format (x_min, y_min, width, height).\n",
    "        penalty_factor (float): A factor to control how much distance affects the score.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Top-left corner of the best matching region in the target image, penalized match score.\n",
    "    \"\"\"\n",
    "    # Convert both the template and target image to grayscale\n",
    "    template_gray = cv2.cvtColor(template, cv2.COLOR_RGB2GRAY)\n",
    "    target_image_gray = cv2.cvtColor(target_image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Perform template matching using cv2.matchTemplate\n",
    "    result = cv2.matchTemplate(target_image_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "    \n",
    "    # Find the location with the highest match score\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    \n",
    "    # max_loc is the top-left corner of the best match\n",
    "    matched_top_left = max_loc\n",
    "    \n",
    "    # Extract the top-left corner of the original template's bounding box\n",
    "    template_top_left = (template_bbox[0], template_bbox[1])\n",
    "    \n",
    "    # Calculate the Euclidean distance between the matched location and the template's original location\n",
    "    distance = np.linalg.norm(np.array(matched_top_left) - np.array(template_top_left))\n",
    "    \n",
    "    # Apply a penalty to the match score based on the distance\n",
    "    penalty = 1 / (1 + penalty_factor * distance)\n",
    "    penalized_score = max_val * penalty\n",
    "    if(penalized_score < distance_threshold):\n",
    "        print(f'Score:{max_val} - Penalty score:{penalized_score}. Skipping')\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_top_left, penalized_score\n",
    "\n",
    "def process_all_masks(image, masks, target_image):\n",
    "    \"\"\"\n",
    "    Process all masks, extract segments from the base image, and find corresponding matching regions in the target image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The input base image.\n",
    "        masks (list): List of SAM-generated mask results (each containing 'segmentation').\n",
    "        target_image (numpy.ndarray): The target image to search for matching regions.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries with information about each match.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Loop over all the masks\n",
    "    for idx, mask_data in enumerate(masks):\n",
    "        # Extract the segmentation mask from the mask_data\n",
    "        mask = mask_data['segmentation']\n",
    "        bbox = mask_data['bbox']\n",
    "        # Extract the segment from the base image using the mask\n",
    "        extracted_segment = image[int(bbox[1]):int(bbox[1]+bbox[3]+1), int(bbox[0]):int(bbox[0]+bbox[2]+1)]\n",
    "        # Find the corresponding matching region in the target image\n",
    "        best_match_loc, match_score = find_matching_segment_with_distance_penalty(extracted_segment, target_image, bbox, penalty_factor=match_distance_penalty_factor, distance_threshold=match_distance_threshold)\n",
    "        if(best_match_loc is None):\n",
    "            continue\n",
    "        \n",
    "        # Store the result with necessary information\n",
    "        results.append({\n",
    "            'mask_index': idx,\n",
    "            'best_match_loc': best_match_loc,\n",
    "            'match_score': match_score,\n",
    "            'segment_shape': extracted_segment.shape[:2]  # Height, width of the segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_matches_side_by_side(base_image_name, target_image_name, base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Plot the original base image with segments on the left and the matched segments on the target image on the right.\n",
    "\n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The original base image from which segments were extracted.\n",
    "        target_image (numpy.ndarray): The target image where matches were found.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'segmentation' and 'bbox').\n",
    "    \"\"\"\n",
    "    # Create a copy of both images for displaying\n",
    "    base_image_copy = base_image.copy()\n",
    "    target_image_copy = target_image.copy()\n",
    "    \n",
    "    # Create a matplotlib figure with two subplots (side by side)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Plot the base image with segment outlines on the left\n",
    "    axes[0].imshow(base_image_copy)\n",
    "    axes[0].set_title(f\"Original {base_image_name} with Segments\")\n",
    "    \n",
    "    # Plot the target image with match rectangles on the right\n",
    "    axes[1].imshow(target_image_copy)\n",
    "    axes[1].set_title(f\"Matched Segments on {target_image_name}\")\n",
    "    \n",
    "    # Loop through the match results and draw bounding boxes for both images\n",
    "    for idx, (result, mask_data) in enumerate(zip(match_results, masks)):\n",
    "        # Extract the original mask and bounding box (for the base image)\n",
    "        mask_bbox = mask_data['bbox']\n",
    "        x_min, y_min, width, height = mask_bbox\n",
    "        \n",
    "        # Draw rectangle and index in the center of the segment in the base image\n",
    "        rect_base = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        axes[0].add_patch(rect_base)\n",
    "        \n",
    "        # Calculate center of the bounding box\n",
    "        center_x_base = x_min + width / 2\n",
    "        center_y_base = y_min + height / 2\n",
    "        \n",
    "        # Add index in the center of the base image's bounding box\n",
    "        axes[0].text(center_x_base, center_y_base, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "\n",
    "        # Draw rectangles around the best match location on the target image\n",
    "        top_left = result['best_match_loc']\n",
    "        h, w = result['segment_shape']  # Height and width of the segment\n",
    "        rect_target = patches.Rectangle(top_left, w, h, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        axes[1].add_patch(rect_target)\n",
    "        \n",
    "        # Calculate center of the bounding box on the target image\n",
    "        center_x_target = top_left[0] + w / 2\n",
    "        center_y_target = top_left[1] + h / 2\n",
    "        \n",
    "        # Add index in the center of the target image's bounding box\n",
    "        axes[1].text(center_x_target, center_y_target, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    # Hide axis ticks for both subplots\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Plot the original base image with segments on the left and the matched segments on the target image on the right.\n",
    "\n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The original base image from which segments were extracted.\n",
    "        target_image (numpy.ndarray): The target image where matches were found.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'segmentation' and 'bbox').\n",
    "    \"\"\"\n",
    "    # Create a copy of both images for displaying\n",
    "    base_image_copy = base_image.copy()\n",
    "    target_image_copy = target_image.copy()\n",
    "    warped_image_copy = warped_image.copy()\n",
    "    # Create a matplotlib figure with three subplots (side by side)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 8))\n",
    "    \n",
    "    # Plot the base image with segment outlines on the left\n",
    "    axes[0].imshow(base_image_copy)\n",
    "    axes[0].set_title(f\"Original {base_image_name} with Segments\")\n",
    "    \n",
    "    # Plot the target image with match rectangles \n",
    "    axes[1].imshow(target_image_copy)\n",
    "    axes[1].set_title(f\"Matched Segments on {target_image_name}\")\n",
    "    \n",
    "    # Plot the warped image on the right\n",
    "    axes[2].imshow(warped_image_copy)\n",
    "    axes[2].set_title(f\"Warped {target_image_name}\")\n",
    "    \n",
    "    # Loop through the match results and draw bounding boxes for base and target image\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "\n",
    "    # for idx, (result, mask_data) in enumerate(zip(match_results, masks)):\n",
    "    for idx, (result, mask_data) in enumerate(zip(match_results, template_bboxes)):\n",
    "        print(f\"{idx} - mask bbox:{mask_data} - best_match_loc:{result['best_match_loc']} \")\n",
    "        # Extract the original mask and bounding box (for the base image)\n",
    "        # mask_bbox = mask_data['bbox']\n",
    "        mask_bbox = mask_data\n",
    "        x_min, y_min, width, height = mask_bbox\n",
    "        \n",
    "        # Draw rectangle and index in the center of the segment in the base image\n",
    "        rect_base = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        axes[0].add_patch(rect_base)\n",
    "        \n",
    "        # Calculate center of the bounding box\n",
    "        center_x_base = x_min + width / 2\n",
    "        center_y_base = y_min + height / 2\n",
    "        \n",
    "        # Add index in the center of the base image's bounding box\n",
    "        axes[0].text(center_x_base, center_y_base, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "\n",
    "        # Draw rectangles around the best match location on the target image\n",
    "        top_left = result['best_match_loc']\n",
    "        h, w = result['segment_shape']  # Height and width of the segment\n",
    "        rect_target = patches.Rectangle(top_left, w, h, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        axes[1].add_patch(rect_target)\n",
    "        \n",
    "        # Calculate center of the bounding box on the target image\n",
    "        center_x_target = top_left[0] + w / 2\n",
    "        center_y_target = top_left[1] + h / 2\n",
    "        \n",
    "        # Add index in the center of the target image's bounding box\n",
    "        axes[1].text(center_x_target, center_y_target, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    # Hide axis ticks\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "            \n",
    "def calculate_bounding_box_center(bbox):\n",
    "    \"\"\"\n",
    "    Calculate the center of a bounding box.\n",
    "    \n",
    "    Args:\n",
    "        bbox (tuple): Bounding box in the format (x_min, y_min, width, height).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Center point (x, y) of the bounding box.\n",
    "    \"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    center_x = x_min + width / 2\n",
    "    center_y = y_min + height / 2\n",
    "    return center_x, center_y\n",
    "\n",
    "def calculate_homography(template_bboxes, target_bboxes):\n",
    "    \"\"\"\n",
    "    Calculate the homography matrix to warp the target image to match the template.\n",
    "    \n",
    "    Args:\n",
    "        template_bboxes (list of tuples): List of bounding boxes in the template image (x_min, y_min, width, height).\n",
    "        target_bboxes (list of tuples): List of bounding boxes in the target image (x_min, y_min, width, height).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The 3x3 homography matrix.\n",
    "    \"\"\"\n",
    "    # Calculate the centers of the bounding boxes\n",
    "    template_points = np.array([calculate_bounding_box_center(bbox) for bbox in template_bboxes])\n",
    "    target_points = np.array([calculate_bounding_box_center(bbox) for bbox in target_bboxes])\n",
    "    \n",
    "    # Find the homography matrix using the points\n",
    "    H, status = cv2.findHomography(target_points, template_points, cv2.RANSAC)\n",
    "    \n",
    "    return H\n",
    "\n",
    "def warp_target_image(target_image, homography_matrix, template_image_size):\n",
    "    \"\"\"\n",
    "    Warp the target image using the homography matrix.\n",
    "    \n",
    "    Args:\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        homography_matrix (numpy.ndarray): The 3x3 homography matrix.\n",
    "        template_image_size (tuple): The size of the template image (width, height).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The warped target image.\n",
    "    \"\"\"\n",
    "    # Warp the target image to align with the template\n",
    "    warped_image = cv2.warpPerspective(target_image, homography_matrix, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def align_images_using_homography(base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Align the target image to the base image using homography based on matched bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The base image (template).\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'bbox' key).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The warped target image.\n",
    "    \"\"\"\n",
    "    # Extract the bounding boxes from the masks and match results\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    print(f\"Building homograph from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "    # Calculate the homography matrix\n",
    "    if(len(template_bboxes) < 4):\n",
    "        print(f\"Not enough matches to warp\")\n",
    "        return None\n",
    "    H = calculate_homography(template_bboxes, target_bboxes)\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Warp the target image to align with the base image\n",
    "    warped_image = warp_target_image(target_image, H, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def get_bbox_corners(bbox):\n",
    "    \"\"\"Returns the four corners of the bounding box.\"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    return [\n",
    "        (x, y),               # Top-left\n",
    "        (x + w, y),           # Top-right\n",
    "        (x, y + h),           # Bottom-left\n",
    "        (x + w, y + h)        # Bottom-right\n",
    "    ]\n",
    "\n",
    "def align_images_using_homography_corners(base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Align the target image to the base image using homography based on matched bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The base image (template).\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'bbox' key).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The warped target image.\n",
    "    \"\"\"\n",
    "    # Extract the bounding boxes from the masks and match results\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    \n",
    "    print(f\"Building homograph from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "\n",
    "    # Use the corners of the bounding boxes instead of just the center\n",
    "    template_points = []\n",
    "    target_points = []\n",
    "\n",
    "    for template_bbox, target_bbox in zip(template_bboxes, target_bboxes):\n",
    "        template_points.extend(get_bbox_corners(template_bbox))\n",
    "        target_points.extend(get_bbox_corners(target_bbox))\n",
    "        \n",
    "    # Convert points to numpy arrays with the correct shape for cv2.findHomography\n",
    "    template_points = np.array(template_points, dtype=np.float32)\n",
    "    target_points = np.array(target_points, dtype=np.float32)        \n",
    "\n",
    "    # Calculate the homography matrix\n",
    "    if len(template_points) < 4:\n",
    "        print(f\"Not enough matches to warp\")\n",
    "        return None\n",
    "\n",
    "    # H = calculate_homography(template_points, target_points)\n",
    "    H, status = cv2.findHomography(target_points, template_points, cv2.RANSAC)\n",
    "\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Warp the target image to align with the base image\n",
    "    warped_image = warp_target_image(target_image, H, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def mask_area_filter(image,masks,min_surf=min_segment_surface, max_surf=max_segment_surface):\n",
    "    surface=image.shape[0]*image.shape[(1)]\n",
    "    return [m for m in masks if ((m['area'] / surface) > min_surf and (m['area'] / surface) < max_surf)]\n",
    "\n",
    "def match_segments(image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(image, filtered_masks, target_image)\n",
    "    return results,filtered_masks\n",
    "\n",
    "def match_and_plot_segments(base_image_name,target_image_name, image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(image, filtered_masks, target_image)\n",
    "    plot_matches_side_by_side(base_image_name,target_image_name, image, target_image, results, filtered_masks)\n",
    "    return results,filtered_masks\n",
    "\n",
    "def match_warp_and_plot_segments(base_image_name, target_image_name, base_image,masks,target_image,min_segment_area=0.0015,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "    if(len(results) > 3):\n",
    "        warped_image = align_images_using_homography(base_image, target_image, results, filtered_masks)\n",
    "    else:\n",
    "        print(f'Only {len(results)} matches. Using bboxes in stead of centers')\n",
    "        warped_image = align_images_using_homography_corners(base_image, target_image, results, filtered_masks)\n",
    "    if(warped_image is not None):\n",
    "        plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, results, filtered_masks)\n",
    "    return results,filtered_masks,warped_image\n",
    "\n",
    "def create_stop_motion_movie(input_folder, output_file, frame_duration=2, transition_duration=1, fps=30):\n",
    "    # Get all jpg files in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()  # Sort the files to ensure correct order\n",
    "    print(f\"files {image_files}\")\n",
    "\n",
    "    # Get the dimensions of the first image\n",
    "    first_image = cv2.imread(os.path.join(input_folder, image_files[0]))\n",
    "    height, width = first_image.shape[:2]\n",
    "\n",
    "    # Create a temporary video file\n",
    "    temp_output = 'temp_output.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))\n",
    "\n",
    "    for i in range(len(image_files)):\n",
    "        current_img = cv2.imread(os.path.join(input_folder, image_files[i]))\n",
    "        next_img = cv2.imread(os.path.join(input_folder, image_files[(i + 1) % len(image_files)]))\n",
    "        \n",
    "        # Hold the current image\n",
    "        for _ in range(fps * frame_duration):\n",
    "            out.write(current_img)\n",
    "        \n",
    "        # Cross-fade to the next image\n",
    "        for j in range(int(fps * transition_duration)):\n",
    "            alpha = j / (fps * transition_duration)\n",
    "            blended = cv2.addWeighted(current_img, 1 - alpha, next_img, alpha, 0)\n",
    "            out.write(blended)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    # Use FFmpeg to convert the temporary video to the final output with improved compression\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', temp_output,\n",
    "        '-c:v', 'libx264',\n",
    "        '-preset', 'slow',\n",
    "        '-crf', '23',\n",
    "        '-vf', f'scale=-2:720',  # Scale to 720p, maintaining aspect ratio\n",
    "        '-movflags', '+faststart',\n",
    "        '-c:a', 'aac',\n",
    "        '-b:a', '128k',\n",
    "        output_file\n",
    "    ]\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_output)\n",
    "\n",
    "    print(f\"Stop motion movie created: {output_file}\")\n",
    "\n",
    "def plot_overlayed(image,masks):\n",
    "    filtered_masks = mask_area_filter(image,masks,0.0,1.0)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    show_anns(filtered_masks)\n",
    "    plt.axis('off')\n",
    "    plt.show() \n",
    "\n",
    "def crop_center(image, crop_width, crop_height):\n",
    "    \"\"\"Crops the image from the center with given width and height.\"\"\"\n",
    "    img_width, img_height = image.size\n",
    "    left = (img_width - crop_width) // 2\n",
    "    top = (img_height - crop_height) // 2\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    return image.crop((left, top, right, bottom))\n",
    "\n",
    "def add_timestamp_from_exif(orig_image_path,image):\n",
    "    # Timestamp from exif\n",
    "    orig_image = Image.open(orig_image_path)\n",
    "    exif_data = piexif.load(orig_image.info.get('exif', b''))\n",
    "    exif_datetime = exif_data.get('Exif', {}).get(piexif.ExifIFD.DateTimeOriginal)\n",
    "    if exif_datetime:\n",
    "        dt = datetime.strptime(exif_datetime.decode('utf-8'), '%Y:%m:%d %H:%M:%S')\n",
    "        timestamp_str = dt.strftime('%d-%b %H:%M')\n",
    "\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(rgb_image)\n",
    "\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        font_path = \"/Library/Fonts/PTSans-Regular.ttf\"\n",
    "        font_size = 40\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "        img_width, img_height = image.size\n",
    "        text_position = (img_width // 2, 80)  # Top center, 12 pixels from the top\n",
    "        draw.text(text_position, timestamp_str, font=font, fill=(255, 255, 255), anchor=\"ms\")\n",
    "        return image\n",
    "    else:\n",
    "        print('No exif datatime for')\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1848a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "mask_generator_1 = SAM2AutomaticMaskGenerator(sam2)\n",
    "mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    points_per_side=32,               # points_per_side: Optional[int] = 32,\n",
    "    points_per_batch=64,              # points_per_batch: int = 64,\n",
    "    pred_iou_thresh=0.8,              # pred_iou_thresh: float = 0.8,\n",
    "    stability_score_thresh=0.95,      # stability_score_thresh: float = 0.95,\n",
    "    stability_score_offset=1.0,       # stability_score_offset: float = 1.0,\n",
    "    crop_n_layers=0,                  # crop_n_layers: int = 0,\n",
    "    box_nms_thresh=0.7,               # box_nms_thresh: float = 0.7,\n",
    "    crop_n_points_downscale_factor=1, # crop_n_points_downscale_factor: int = 1,\n",
    "    min_mask_region_area=5.0,        # min_mask_region_area: int = 0,\n",
    "    use_m2m=False,                     # use_m2m: bool = False,\n",
    ")\n",
    "\n",
    "mask_generator=mask_generator_2\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.jpg')])\n",
    "# image_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.jpg') and (f==base_image or f=='IMG_20240528_080644.jpg')])\n",
    "images_list = [load_image(os.path.join(folder_path, file)) for file in image_files]\n",
    "print(f'Loaded {len(images_list)} images from {folder_path}')\n",
    "\n",
    "base_image_ix = image_files.index(base_image)\n",
    "masks_base_image = mask_generator.generate(images_list[base_image_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5de70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_0 = expand_bounding_boxes_in_masks(images_list[base_image_ix], masks_base_image, matched_segments_bbox_expand)\n",
    "\n",
    "matched_results=[]\n",
    "matched_masks=[]\n",
    "\n",
    "images_to_warp = images_list[:len(images_list)]\n",
    "\n",
    "## Choose one of 1|2|3\n",
    "# 1. Match segments\n",
    "# for idx, image in enumerate(images_to_warp):\n",
    "#     results, masks = match_segments(images_list[base_image_ix], masks_0, image)\n",
    "#     matched_results.append(results)\n",
    "#     matched_masks.append(masks)\n",
    "\n",
    "# 2. Match and plot matched segments\n",
    "# for idx, image in enumerate(images_to_warp):\n",
    "#     results, masks = match_and_plot_segments(image_files[base_image_ix],image_files[idx], images_list[base_image_ix], masks_0, image)\n",
    "#     matched_results.append(results)\n",
    "#     # matched_top_masks.append(masks)\n",
    "#     matched_masks.append(masks)\n",
    "\n",
    "\n",
    "# 3. Match and warp and plot matched segments and warped image\n",
    "for idx, image in enumerate(images_to_warp):\n",
    "    print(f\"Working on {image_files[idx]}\")\n",
    "    results, masks, warped_image = match_warp_and_plot_segments(image_files[base_image_ix],image_files[idx],  images_list[base_image_ix], masks_0, image)\n",
    "    matched_results.append(results)\n",
    "    matched_masks.append(masks)\n",
    "    if(warped_image is not None):\n",
    "        warped_image_bgr = cv2.cvtColor(warped_image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(f'./auto_warped/{image_files[idx]}', warped_image_bgr)   \n",
    "    else:\n",
    "        print(f\"Cant't warp {image_files[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210f568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "for ix, image in enumerate(images_to_warp):\n",
    "    if(len(matched_results[ix]) > 3):\n",
    "        warped_image = align_images_using_homography(images_list[base_image_ix], images_list[ix], matched_results[ix], matched_masks[ix])\n",
    "    else:\n",
    "        print(f'Only {len(matched_results[ix])} matches. Using bboxes in stead of centers')\n",
    "        warped_image = align_images_using_homography_corners(images_list[base_image_ix], images_list[ix], matched_results[ix], matched_masks[ix])\n",
    "    \n",
    "    if(warped_image is not None):\n",
    "        rgb_image = cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(rgb_image)\n",
    "        cropped_img = crop_center(pil_image, 3000, 1800)\n",
    "        opencv_cropped_img = np.array(cropped_img)\n",
    "        opencv_cropped_img_bgr = opencv_cropped_img\n",
    "        final_img = add_timestamp_from_exif(f\"{folder_path}/{image_files[ix]}\", opencv_cropped_img_bgr)\n",
    "        final_img.save(f'{warped_images_folder}/{image_files[ix]}')\n",
    "    else:\n",
    "        print(f\"Cant't warp {image_files[ix]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e316a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import datetime\n",
    "import locale\n",
    "\n",
    "def create_stop_motion_movie_with_steps(input_folder, output_file, csv_file, frame_duration=2, transition_duration=1, fps=30, audio_file=None):\n",
    "    # Read CSV data\n",
    "    data = pd.read_csv(csv_file, parse_dates=['date'])\n",
    "\n",
    "    # Get all jpg files in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()  # Sort the files to ensure correct order\n",
    "    print(f\"files {image_files}\")\n",
    "\n",
    "    # Get the dimensions of the first image\n",
    "    first_image = cv2.imread(os.path.join(input_folder, image_files[0]))\n",
    "    height, width = first_image.shape[:2]\n",
    "\n",
    "    # Create a temporary video file\n",
    "    temp_output = 'temp_output.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize the line chart image\n",
    "    line_chart_file = 'line_chart.png'\n",
    "    full_data = data.copy()\n",
    "\n",
    "    # Loop through each image\n",
    "    for i in range(len(image_files)):\n",
    "        current_img = cv2.imread(os.path.join(input_folder, image_files[i]))\n",
    "        next_img = cv2.imread(os.path.join(input_folder, image_files[(i + 1) % len(image_files)]))\n",
    "        \n",
    "        # Resize images to ensure they have the same size\n",
    "        current_img = cv2.resize(current_img, (width, height))\n",
    "        next_img = cv2.resize(next_img, (width, height))\n",
    "\n",
    "        # Extract the date from the filename (format: IMG_yyyymmdd_hhmmss.jpg)\n",
    "        image_date_str = image_files[i][4:12]  # Extract yyyymmdd\n",
    "        image_date = datetime.strptime(image_date_str, '%Y%m%d')\n",
    "\n",
    "        # Filter the CSV data up to the current image date\n",
    "        filtered_data = data[data['date'] <= image_date]\n",
    "\n",
    "        # Update the line chart with the filtered data (this keeps the line chart updated with new data)\n",
    "        create_line_chart(filtered_data, full_data, line_chart_file)\n",
    "\n",
    "        # Superimpose the line chart on the current image (without blending it)\n",
    "        current_img_with_chart = overlay_line_chart(current_img, line_chart_file)\n",
    "\n",
    "        # Hold the current image (with the updated chart)\n",
    "        for _ in range(fps * frame_duration):\n",
    "            out.write(current_img_with_chart)\n",
    "\n",
    "        # Cross-fade the images, but without fading the line chart\n",
    "        for j in range(int(fps * transition_duration)):\n",
    "            alpha = j / (fps * transition_duration)\n",
    "            blended_img = cv2.addWeighted(current_img, 1 - alpha, next_img, alpha, 0)\n",
    "            # Overlay the line chart on the blended image (to keep it static during the transition)\n",
    "            blended_img_with_chart = overlay_line_chart(blended_img, line_chart_file)\n",
    "            out.write(blended_img_with_chart)\n",
    "\n",
    "    # Hold the last image if audio is longer than the video\n",
    "    last_img = current_img_with_chart  # The last image with the chart\n",
    "    video_duration = len(image_files) * (frame_duration + transition_duration)  # In seconds\n",
    "\n",
    "    # Get the duration of the MP3 audio file\n",
    "    if audio_file:\n",
    "        result = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', audio_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        audio_duration = float(result.stdout)\n",
    "        \n",
    "        if audio_duration > video_duration:\n",
    "            hold_duration = audio_duration - video_duration\n",
    "            print(f\"Holding the last frame for {hold_duration} seconds.\")\n",
    "            # Hold the last image for the remaining duration of the audio\n",
    "            for _ in range(int(hold_duration * fps)):\n",
    "                out.write(last_img)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    # FFmpeg command to add the MP3 audio to the video\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', temp_output,            # Input: stop motion movie\n",
    "        '-i', audio_file,             # Input: MP3 file\n",
    "        '-c:v', 'libx264',\n",
    "        '-preset', 'slow',\n",
    "        '-crf', '23',\n",
    "        '-vf', f'scale=-2:720',  # Scale to 720p, maintaining aspect ratio\n",
    "        '-movflags', '+faststart',\n",
    "        '-c:a', 'aac',\n",
    "        '-b:a', '128k',\n",
    "        '-ar', '44100',            # Resample audio to 44.1 kHz\n",
    "        '-shortest',               # Stops the video when the shorter stream ends (video or audio)\n",
    "        output_file\n",
    "    ]\n",
    "\n",
    "    # Run the FFmpeg command\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_output)\n",
    "\n",
    "    print(f\"Stop motion movie with audio created: {output_file}\")\n",
    "\n",
    "def overlay_line_chart(image, line_chart_file):\n",
    "    \"\"\"\n",
    "    Overlay the line chart in the bottom-right corner of the given image,\n",
    "    resizing it to 750px wide and 450px high.\n",
    "    \"\"\"\n",
    "    # Load the line chart as an image (with alpha channel for transparency)\n",
    "    line_chart_img = cv2.imread(line_chart_file, cv2.IMREAD_UNCHANGED)  # Load with transparency\n",
    "\n",
    "    # Resize the line chart to be 750px wide and 450px high\n",
    "    line_chart_img_resized = cv2.resize(line_chart_img, (750, 450))\n",
    "\n",
    "    # Get the dimensions of the resized chart\n",
    "    chart_height, chart_width = line_chart_img_resized.shape[:2]\n",
    "\n",
    "    # Define the region of interest (ROI) in the bottom-right corner\n",
    "    x_offset = image.shape[1] - chart_width - 10  # 10 pixels from the right\n",
    "    y_offset = image.shape[0] - chart_height - 10  # 10 pixels from the bottom\n",
    "\n",
    "    # If the line chart has an alpha channel, blend it with the image\n",
    "    if line_chart_img_resized.shape[2] == 4:\n",
    "        # Split the line chart into its color channels and alpha channel\n",
    "        b, g, r, alpha = cv2.split(line_chart_img_resized)\n",
    "\n",
    "        # Normalize the alpha channel to be between 0 and 1\n",
    "        alpha = alpha / 255.0\n",
    "\n",
    "        # Blend the line chart with the image\n",
    "        for c in range(0, 3):  # Iterate over the B, G, R channels\n",
    "            image[y_offset:y_offset+chart_height, x_offset:x_offset+chart_width, c] = (\n",
    "                alpha * line_chart_img_resized[:, :, c] +\n",
    "                (1 - alpha) * image[y_offset:y_offset+chart_height, x_offset:x_offset+chart_width, c]\n",
    "            )\n",
    "\n",
    "    return image\n",
    "\n",
    "def create_line_chart(filtered_data, full_data, output_file):\n",
    "    \"\"\"\n",
    "    Creates a static line chart from filtered data (up to a specific date) and saves it as an image.\n",
    "    The chart will have a light grey background with 70% opacity, display steps in 1K units with 1 decimal,\n",
    "    and annotate the last data point with the date in Dutch format (dd-mon).\n",
    "    \"\"\"\n",
    "    # Set locale to Dutch\n",
    "    locale.setlocale(locale.LC_TIME, 'nl_NL.UTF-8')  # For Dutch date formatting\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))  # Create a smaller chart (400x200px)\n",
    "\n",
    "    # Set light grey background for the figure with 70% opacity\n",
    "    fig.patch.set_facecolor(mcolors.to_rgba('lightgrey', 0.7))\n",
    "    ax.set_facecolor(mcolors.to_rgba('lightgrey', 0.7))\n",
    "\n",
    "    # Plot the filtered data\n",
    "    ax.plot(filtered_data['date'], filtered_data['steps'], lw=2)\n",
    "\n",
    "    # Calculate 5% margin for x-axis extension\n",
    "    date_range = full_data['date'].max() - full_data['date'].min()\n",
    "    margin = date_range * 0.10  # 5% of the date range\n",
    "\n",
    "    # Set the x-axis limits with the added margin\n",
    "    ax.set_xlim(full_data['date'].min(), full_data['date'].max() + margin)\n",
    "\n",
    "    # Set the y-axis limits\n",
    "    ax.set_ylim(full_data['steps'].min() * 0.9, full_data['steps'].max() * 1.1)\n",
    "\n",
    "    # Format y-axis to show steps in 1K units with 1 decimal\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.1f}K'))\n",
    "\n",
    "    # Annotate the last data point if available, also in 1K units\n",
    "    if not filtered_data.empty:\n",
    "        last_date = filtered_data['date'].iloc[-1]\n",
    "        last_value = filtered_data['steps'].iloc[-1] / 1000  # Convert to 1K units\n",
    "        last_date_str = last_date.strftime('%d %b')  # Format date as 'dd-mon' in Dutch\n",
    "\n",
    "        ax.annotate(f'{last_value:.1f}K\\n{last_date_str}',  # Display value in 1K units with 1 decimal and date\n",
    "                    xy=(last_date, last_value * 1000),  # Point at the last data point in original scale\n",
    "                    xytext=(5, 5),  # Slightly offset the text\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10,\n",
    "                    color='black')\n",
    "\n",
    "    # Remove axis lines and labels\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Save the chart as an image with a semi-transparent background\n",
    "    plt.savefig(output_file, dpi=100)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619522ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(warped_images_folder)\n",
    "# Stop motion from warped images\n",
    "input_folder = warped_images_folder\n",
    "output_file = stop_motion_result_path\n",
    "create_stop_motion_movie_with_steps(warped_images_folder, output_file, \"steps.csv\", transition_duration=0.75, audio_file='one_fine_day.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53009a1f",
   "metadata": {},
   "source": [
    "Show all the masks overlayed on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=10\n",
    "plot_overlayed(images_list[base_image_ix],masks_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
