{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2bc687",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effad654-436d-400e-97cc-8bd36141370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import subprocess\n",
    "import piexif\n",
    "from datetime import datetime\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "import pickle \n",
    "from typing import List, Tuple, Any\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# MPS seems to crash every now and then\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73bc43",
   "metadata": {},
   "source": [
    "<h1>Config</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae5ce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params stadspark\n",
    "base_path='/Users/peter/playground/parkbridge'\n",
    "src_image_path = f'{base_path}/source_images'\n",
    "test_image_path = f'{base_path}/subset_test_images'\n",
    "\n",
    "image_path = test_image_path\n",
    "masks_path = f'{image_path}_masks'\n",
    "\n",
    "homograpy_warped_images_folder = f'{base_path}/auto_warped_homography'\n",
    "cropped_with_exif_images_folder = f'{base_path}/auto_warped_homography_cropped_with_exif'\n",
    "translation_warped_images_folder = f'{base_path}/auto_warped_translation'\n",
    "translation_and_rotation_warped_images_folder = f'{base_path}/auto_warped_translation_and_rotation'\n",
    "\n",
    "# base_image_1 = 'IMG_20240820_074416.jpg'\n",
    "# base_image_2 = 'IMG_20241120_123340.jpg'\n",
    "# base_image_1 = 'IMG_20240528_080644.jpg'\n",
    "# base_image_2 = 'IMG_20240528_080644_manual.jpg'\n",
    "\n",
    "stop_motion_result_path=f'{base_path}/auto_parkbridge.mp4'\n",
    "\n",
    "crop = True\n",
    "crop_width = 3000\n",
    "crop_height = 1800\n",
    "\n",
    "matched_segments_bbox_expand = 0.05\n",
    "min_segment_surface = 0.005\n",
    "max_segment_surface = 0.075\n",
    "match_distance_penalty_factor=0.075\n",
    "match_distance_threshold=0.015\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12923056",
   "metadata": {},
   "source": [
    "<h1>Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74b6e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask \n",
    "        if borders:\n",
    "            import cv2\n",
    "            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "            # Try to smooth contours\n",
    "            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "    ax.imshow(img)\n",
    "\n",
    "def downscale_image_by_percentage(image, scale_percent):\n",
    "    \"\"\"\n",
    "    Downscale the image by a percentage while maintaining the aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image or numpy.ndarray): The input image to downscale.\n",
    "        scale_percent (float): The percentage to scale the image by (e.g., 50 for 50% of the original size).\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The downscaled image.\n",
    "    \"\"\"\n",
    "    # If the image is in NumPy format, convert it back to a PIL Image\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "    # Calculate the new size based on the scale percentage\n",
    "    width, height = image.size\n",
    "    new_width = int(width * scale_percent / 100)\n",
    "    new_height = int(height * scale_percent / 100)\n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Resize the image to the new size\n",
    "    downscaled_image = image.resize(new_size,  Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return downscaled_image\n",
    "\n",
    "def _load_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = downscale_image_by_percentage(image, scale_percent=100)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    return image\n",
    "\n",
    "def plt_image(image):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def expand_bounding_boxes_in_masks(image, masks_input, expand_percent=0.0):\n",
    "    \"\"\"\n",
    "    Expand the bounding boxes of all masks by a given percentage, while ensuring they stay within image boundaries,\n",
    "    and update the masks in place with the expanded bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        masks (list): List of masks generated by SAM, each with a 'bbox' key.\n",
    "        image (numpy.ndarray): The image from which the bounding box is extracted (to get dimensions).\n",
    "        expand_percent (float): The percentage by which to expand the bounding boxes (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        list: The updated list of masks with modified 'bbox' values.\n",
    "    \"\"\"\n",
    "    masks = copy.deepcopy(masks_input)\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    image_height, image_width = image.shape[:2]  # Height and width from the image dimensions\n",
    "    \n",
    "    # Iterate over all masks and expand their bounding boxes\n",
    "    for mask in masks:\n",
    "        x_min, y_min, width, height = mask['bbox']\n",
    "        \n",
    "        # Calculate the amount to expand in each direction\n",
    "        expand_w = width * expand_percent\n",
    "        expand_h = height * expand_percent\n",
    "        \n",
    "        # Calculate the new bounding box\n",
    "        new_x_min = max(0, x_min - expand_w / 2)  # Ensure it doesn't go below 0\n",
    "        new_y_min = max(0, y_min - expand_h / 2)  # Ensure it doesn't go below 0\n",
    "        \n",
    "        new_x_max = min(image_width, x_min + width + expand_w / 2)  # Ensure it doesn't exceed image width\n",
    "        new_y_max = min(image_height, y_min + height + expand_h / 2)  # Ensure it doesn't exceed image height\n",
    "        \n",
    "        # Calculate new width and height based on the expanded coordinates\n",
    "        new_width = new_x_max - new_x_min\n",
    "        new_height = new_y_max - new_y_min\n",
    "        \n",
    "        # Update the 'bbox' in the mask with the expanded bounding box\n",
    "        mask['bbox'] = (new_x_min, new_y_min, new_width, new_height)\n",
    "    \n",
    "    # Return the updated list of masks\n",
    "    return masks\n",
    "\n",
    "def find_matching_segment_with_distance_penalty(template, target_image, template_bbox, penalty_factor, distance_threshold):\n",
    "    \"\"\"\n",
    "    Use cv2.matchTemplate to find the location of the segment in the target image,\n",
    "    and penalize the match score based on how far the match is from the original template's bounding box.\n",
    "    \n",
    "    Args:\n",
    "        template (numpy.ndarray): The extracted segment (template) from the first image.\n",
    "        target_image (numpy.ndarray): The target image in which to search for the template.\n",
    "        template_bbox (tuple): The bounding box of the template in the format (x_min, y_min, width, height).\n",
    "        penalty_factor (float): A factor to control how much distance affects the score.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Top-left corner of the best matching region in the target image, penalized match score.\n",
    "    \"\"\"\n",
    "    # Convert both the template and target image to grayscale\n",
    "    template_gray = cv2.cvtColor(template, cv2.COLOR_RGB2GRAY)\n",
    "    target_image_gray = cv2.cvtColor(target_image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Perform template matching using cv2.matchTemplate\n",
    "    result = cv2.matchTemplate(target_image_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "    \n",
    "    # Find the location with the highest match score\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    \n",
    "    # max_loc is the top-left corner of the best match\n",
    "    matched_top_left = max_loc\n",
    "    \n",
    "    # Extract the top-left corner of the original template's bounding box\n",
    "    template_top_left = (template_bbox[0], template_bbox[1])\n",
    "    \n",
    "    # Calculate the Euclidean distance between the matched location and the template's original location\n",
    "    distance = np.linalg.norm(np.array(matched_top_left) - np.array(template_top_left))\n",
    "    \n",
    "    # Apply a penalty to the match score based on the distance\n",
    "    penalty = 1 / (1 + penalty_factor * distance)\n",
    "    penalized_score = max_val * penalty\n",
    "    if(penalized_score < distance_threshold):\n",
    "        print(f'Score:{max_val} - Penalty score:{penalized_score}. Skipping')\n",
    "        return None, None\n",
    "    else:\n",
    "        print(f'Score:{max_val} - Penalty score:{penalized_score}. Keeping')\n",
    "        return matched_top_left, penalized_score\n",
    "\n",
    "def process_all_masks(image, masks, target_image):\n",
    "    \"\"\"\n",
    "    Process all masks, extract segments from the base image, and find corresponding matching regions in the target image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The input base image.\n",
    "        masks (list): List of SAM-generated mask results (each containing 'segmentation').\n",
    "        target_image (numpy.ndarray): The target image to search for matching regions.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries with information about each match.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Loop over all the masks\n",
    "    for idx, mask_data in enumerate(masks):\n",
    "        # Extract the segmentation mask from the mask_data\n",
    "        mask = mask_data['segmentation']\n",
    "        bbox = mask_data['bbox']\n",
    "        # Extract the segment from the base image using the mask\n",
    "        extracted_segment = image[int(bbox[1]):int(bbox[1]+bbox[3]+1), int(bbox[0]):int(bbox[0]+bbox[2]+1)]\n",
    "        # Find the corresponding matching region in the target image\n",
    "        best_match_loc, match_score = find_matching_segment_with_distance_penalty(extracted_segment, target_image, bbox, penalty_factor=match_distance_penalty_factor, distance_threshold=match_distance_threshold)\n",
    "        if(best_match_loc is None):\n",
    "            continue\n",
    "        \n",
    "        # Store the result with necessary information\n",
    "        results.append({\n",
    "            'mask_index': idx,\n",
    "            'best_match_loc': best_match_loc,\n",
    "            'match_score': match_score,\n",
    "            'segment_shape': extracted_segment.shape[:2]  # Height, width of the segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_matches_side_by_side(base_image_name, target_image_name, base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Plot the original base image with segments on the left and the matched segments on the target image on the right.\n",
    "\n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The original base image from which segments were extracted.\n",
    "        target_image (numpy.ndarray): The target image where matches were found.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'segmentation' and 'bbox').\n",
    "    \"\"\"\n",
    "    # Create a copy of both images for displaying\n",
    "    base_image_copy = base_image.copy()\n",
    "    target_image_copy = target_image.copy()\n",
    "    \n",
    "    # Create a matplotlib figure with two subplots (side by side)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Plot the base image with segment outlines on the left\n",
    "    axes[0].imshow(base_image_copy)\n",
    "    axes[0].set_title(f\"Original {base_image_name} with Segments\")\n",
    "    \n",
    "    # Plot the target image with match rectangles on the right\n",
    "    axes[1].imshow(target_image_copy)\n",
    "    axes[1].set_title(f\"Matched Segments on {target_image_name}\")\n",
    "    \n",
    "    # Loop through the match results and draw bounding boxes for both images\n",
    "    for idx, (result, mask_data) in enumerate(zip(match_results, masks)):\n",
    "        # Extract the original mask and bounding box (for the base image)\n",
    "        mask_bbox = mask_data['bbox']\n",
    "        x_min, y_min, width, height = mask_bbox\n",
    "        \n",
    "        # Draw rectangle and index in the center of the segment in the base image\n",
    "        rect_base = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        axes[0].add_patch(rect_base)\n",
    "        \n",
    "        # Calculate center of the bounding box\n",
    "        center_x_base = x_min + width / 2\n",
    "        center_y_base = y_min + height / 2\n",
    "        \n",
    "        # Add index in the center of the base image's bounding box\n",
    "        axes[0].text(center_x_base, center_y_base, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "\n",
    "        # Draw rectangles around the best match location on the target image\n",
    "        top_left = result['best_match_loc']\n",
    "        h, w = result['segment_shape']  # Height and width of the segment\n",
    "        rect_target = patches.Rectangle(top_left, w, h, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        axes[1].add_patch(rect_target)\n",
    "        \n",
    "        # Calculate center of the bounding box on the target image\n",
    "        center_x_target = top_left[0] + w / 2\n",
    "        center_y_target = top_left[1] + h / 2\n",
    "        \n",
    "        # Add index in the center of the target image's bounding box\n",
    "        axes[1].text(center_x_target, center_y_target, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    # Hide axis ticks for both subplots\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Plot the original base image with segments on the left and the matched segments on the target image on the right.\n",
    "\n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The original base image from which segments were extracted.\n",
    "        target_image (numpy.ndarray): The target image where matches were found.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'segmentation' and 'bbox').\n",
    "    \"\"\"\n",
    "    # Create a copy of both images for displaying\n",
    "    base_image_copy = base_image.copy()\n",
    "    target_image_copy = target_image.copy()\n",
    "    if(warped_image is not None):\n",
    "        warped_image_copy = warped_image.copy()\n",
    "    else:\n",
    "        warped_image_copy = None\n",
    "    # Create a matplotlib figure with three subplots (side by side)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    \n",
    "    # Plot the base image with segment outlines on the left\n",
    "    axes[0].imshow(base_image_copy)\n",
    "    axes[0].set_title(f\"Original {base_image_name} with Segments\", fontsize=9)\n",
    "    \n",
    "    # Plot the target image with match rectangles \n",
    "    axes[1].imshow(target_image_copy)\n",
    "    axes[1].set_title(f\"Matched Segments on {target_image_name}\", fontsize=9)\n",
    "    \n",
    "    # Plot the warped image on the right\n",
    "    if(warped_image_copy is not None):\n",
    "        axes[2].imshow(warped_image_copy)\n",
    "        axes[2].set_title(f\"Warped {target_image_name}\", fontsize=9)\n",
    "    \n",
    "    # Loop through the match results and draw bounding boxes for base and target image\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "\n",
    "    # for idx, (result, mask_data) in enumerate(zip(match_results, masks)):\n",
    "    for idx, (result, mask_data) in enumerate(zip(match_results, template_bboxes)):\n",
    "        print(f\"{idx} - mask bbox:{mask_data} - best_match_loc:{result['best_match_loc']} \")\n",
    "        # Extract the original mask and bounding box (for the base image)\n",
    "        # mask_bbox = mask_data['bbox']\n",
    "        mask_bbox = mask_data\n",
    "        x_min, y_min, width, height = mask_bbox\n",
    "        \n",
    "        # Draw rectangle and index in the center of the segment in the base image\n",
    "        rect_base = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        axes[0].add_patch(rect_base)\n",
    "        \n",
    "        # Calculate center of the bounding box\n",
    "        center_x_base = x_min + width / 2\n",
    "        center_y_base = y_min + height / 2\n",
    "        \n",
    "        # Add index in the center of the base image's bounding box\n",
    "        axes[0].text(center_x_base, center_y_base, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "\n",
    "        # Draw rectangles around the best match location on the target image\n",
    "        top_left = result['best_match_loc']\n",
    "        h, w = result['segment_shape']  # Height and width of the segment\n",
    "        rect_target = patches.Rectangle(top_left, w, h, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        axes[1].add_patch(rect_target)\n",
    "        \n",
    "        # Calculate center of the bounding box on the target image\n",
    "        center_x_target = top_left[0] + w / 2\n",
    "        center_y_target = top_left[1] + h / 2\n",
    "        \n",
    "        # Add index in the center of the target image's bounding box\n",
    "        axes[1].text(center_x_target, center_y_target, str(idx), color='white', fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    # Hide axis ticks\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "            \n",
    "def calculate_bounding_box_center(bbox):\n",
    "    \"\"\"\n",
    "    Calculate the center of a bounding box.\n",
    "    \n",
    "    Args:\n",
    "        bbox (tuple): Bounding box in the format (x_min, y_min, width, height).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Center point (x, y) of the bounding box.\n",
    "    \"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    center_x = x_min + width / 2\n",
    "    center_y = y_min + height / 2\n",
    "    return center_x, center_y\n",
    "\n",
    "def calculate_homography_metrics(H):\n",
    "    # Scale factors\n",
    "    scale_x = np.sqrt(H[0, 0]**2 + H[0, 1]**2)\n",
    "    scale_y = np.sqrt(H[1, 0]**2 + H[1, 1]**2)\n",
    "    \n",
    "    # Shear\n",
    "    shear = (H[0, 0] * H[1, 0] + H[0, 1] * H[1, 1]) / (scale_x * scale_y)\n",
    "    \n",
    "    # Determinant\n",
    "    determinant = H[0, 0] * H[1, 1] - H[0, 1] * H[1, 0]\n",
    "    \n",
    "    # Condition number\n",
    "    u, s, vh = np.linalg.svd(H[:2, :2])  # Only consider the upper 2x2 for condition number\n",
    "    condition_number = s[0] / s[-1] if s[-1] != 0 else np.inf\n",
    "    \n",
    "    # Perspective components\n",
    "    perspective = np.linalg.norm(H[2, :2])\n",
    "    \n",
    "    # Angular deviation (rotation matrix approximation)\n",
    "    rotation_angle = np.arctan2(H[1, 0], H[0, 0]) * (180 / np.pi)  # Convert to degrees\n",
    "    \n",
    "    return {\n",
    "        \"scale_x\": scale_x,\n",
    "        \"scale_y\": scale_y,\n",
    "        \"shear\": shear,\n",
    "        \"determinant\": determinant,\n",
    "        \"condition_number\": condition_number,\n",
    "        \"perspective\": perspective,\n",
    "        \"rotation_angle\": rotation_angle\n",
    "    }\n",
    "\n",
    "def calculate_rigid_alignment(template_bboxes, target_bboxes):\n",
    "    \"\"\"\n",
    "    Calculate the rigid transformation matrix (translation + rotation) to align\n",
    "    the target image to the template.\n",
    "\n",
    "    Args:\n",
    "        template_bboxes (list of tuples): List of bounding boxes in the template image (x_min, y_min, width, height).\n",
    "        target_bboxes (list of tuples): List of bounding boxes in the target image (x_min, y_min, width, height).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The 3x3 rigid transformation matrix.\n",
    "    \"\"\"\n",
    "    def calculate_bounding_box_center(bbox):\n",
    "        x, y, w, h = bbox\n",
    "        return np.array([x + w / 2, y + h / 2])\n",
    "\n",
    "    # Calculate the centers of the bounding boxes\n",
    "    template_points = np.array([calculate_bounding_box_center(bbox) for bbox in template_bboxes])\n",
    "    target_points = np.array([calculate_bounding_box_center(bbox) for bbox in target_bboxes])\n",
    "\n",
    "    # Centralize the points\n",
    "    template_center = np.mean(template_points, axis=0)\n",
    "    target_center = np.mean(target_points, axis=0)\n",
    "    template_centered = template_points - template_center\n",
    "    target_centered = target_points - target_center\n",
    "\n",
    "    # Singular Value Decomposition (SVD) to compute rotation\n",
    "    U, _, Vt = np.linalg.svd(np.dot(target_centered.T, template_centered))\n",
    "    R = np.dot(U, Vt)\n",
    "\n",
    "    # Ensure no reflection in rotation\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R[:, -1] *= -1\n",
    "\n",
    "    # Compute translation\n",
    "    t = template_center - np.dot(target_center, R)\n",
    "\n",
    "    # Construct the rigid transformation matrix\n",
    "    rigid_transform = np.eye(3)\n",
    "    rigid_transform[:2, :2] = R\n",
    "    rigid_transform[:2, 2] = t\n",
    "\n",
    "    print(f'...rigid transform params: {calculate_homography_metrics(rigid_transform)}')\n",
    "\n",
    "    return rigid_transform\n",
    "\n",
    "def calculate_homography(template_bboxes, target_bboxes):\n",
    "    \"\"\"\n",
    "    Calculate the homography matrix to warp the target image to match the template.\n",
    "    \n",
    "    Args:\n",
    "        template_bboxes (list of tuples): List of bounding boxes in the template image (x_min, y_min, width, height).\n",
    "        target_bboxes (list of tuples): List of bounding boxes in the target image (x_min, y_min, width, height).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The 3x3 homography matrix.\n",
    "    \"\"\"\n",
    "    # Calculate the centers of the bounding boxes\n",
    "    template_points = np.array([calculate_bounding_box_center(bbox) for bbox in template_bboxes])\n",
    "    target_points = np.array([calculate_bounding_box_center(bbox) for bbox in target_bboxes])\n",
    "    \n",
    "    # Find the homography matrix using the points\n",
    "    H, status = cv2.findHomography(target_points, template_points, cv2.RANSAC)\n",
    "    print(f'...homography params: {calculate_homography_metrics(H)}')\n",
    "    \n",
    "    return H\n",
    "\n",
    "def calculate_homography_with_rigid_transform_fallback(template_bboxes, target_bboxes):\n",
    "    # Calculate the centers of the bounding boxes\n",
    "    template_points = np.array([calculate_bounding_box_center(bbox) for bbox in template_bboxes])\n",
    "    target_points = np.array([calculate_bounding_box_center(bbox) for bbox in target_bboxes])\n",
    "    \n",
    "    # Find the homography matrix using the points\n",
    "    H, status = cv2.findHomography(target_points, template_points, cv2.RANSAC)\n",
    "    metrics = calculate_homography_metrics(H)\n",
    "    print(f'...homography params: {metrics}')\n",
    "    if (metrics[\"condition_number\"] > 10 or\n",
    "        metrics[\"scale_x\"] > 2 or metrics[\"scale_y\"] > 2 or\n",
    "        metrics[\"determinant\"] < 0.1 or metrics[\"determinant\"] > 2 or\n",
    "        abs(metrics[\"rotation_angle\"]) > 15):\n",
    "            print(\"WARNING!!!! - distortion too high, falling back to rigid alignment.\")\n",
    "            rah = calculate_rigid_alignment(template_bboxes, target_bboxes)\n",
    "            metrics = calculate_homography_metrics(H)\n",
    "            print(f'...rigid transformation params: {rah}')\n",
    "            return rah\n",
    "    else:\n",
    "        return H\n",
    "\n",
    "def warp_target_image(target_image, homography_matrix, template_image_size):\n",
    "    \"\"\"\n",
    "    Warp the target image using the homography matrix.\n",
    "    \n",
    "    Args:\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        homography_matrix (numpy.ndarray): The 3x3 homography matrix.\n",
    "        template_image_size (tuple): The size of the template image (width, height).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The warped target image.\n",
    "    \"\"\"\n",
    "    # Warp the target image to align with the template\n",
    "    warped_image = cv2.warpPerspective(target_image, homography_matrix, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def align_images_using_homography(base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Align the target image to the base image using homography based on matched bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The base image (template).\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'bbox' key).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The warped target image.\n",
    "    \"\"\"\n",
    "    # Extract the bounding boxes from the masks and match results\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    print(f\"Building homograph from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "    # Calculate the homography matrix\n",
    "    if(len(template_bboxes) < 4):\n",
    "        print(f\"Not enough matches to warp\")\n",
    "        return None\n",
    "    H = calculate_homography(template_bboxes, target_bboxes)\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Warp the target image to align with the base image\n",
    "    warped_image = warp_target_image(target_image, H, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def align_images_using_rigid_transform(base_image, target_image, match_results, masks):\n",
    "    # Extract the bounding boxes from the masks and match results\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    print(f\"Building homograph from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "    H = calculate_rigid_alignment(template_bboxes, target_bboxes)\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Warp the target image to align with the base image\n",
    "    warped_image = warp_target_image(target_image, H, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def align_images_using_homography_with_rigid_transform_fallback(base_image, target_image, match_results, masks):\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    print(f\"Building homograph from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "\n",
    "    if len(template_bboxes) < 4:\n",
    "        print(\"Not enough matches to warp. Falling back to corners.\")\n",
    "        \n",
    "        def get_bbox_corners_as_bboxes(bbox):\n",
    "            x, y, w, h = bbox\n",
    "            return [\n",
    "                (x, y, 0, 0),          # Top-left as a bbox\n",
    "                (x + w, y, 0, 0),      # Top-right as a bbox\n",
    "                (x, y + h, 0, 0),      # Bottom-left as a bbox\n",
    "                (x + w, y + h, 0, 0),  # Bottom-right as a bbox\n",
    "            ]\n",
    "        \n",
    "        # Prepare fallback template and target bboxes\n",
    "        template_points = []\n",
    "        target_points = []\n",
    "        for template_bbox, target_bbox in zip(template_bboxes, target_bboxes):\n",
    "            template_points.extend(get_bbox_corners_as_bboxes(template_bbox))\n",
    "            target_points.extend(get_bbox_corners_as_bboxes(target_bbox))\n",
    "        \n",
    "        print(f\"Using corners as fallback: {len(template_points)} point pairs\")\n",
    "        if len(template_points) < 4:\n",
    "            print(\"Still not enough points to compute homography, even with corners.\")\n",
    "            return None\n",
    "\n",
    "        # Use the fallback points to calculate the homography\n",
    "        H = calculate_homography_with_rigid_transform_fallback(template_points, target_points)\n",
    "    else:\n",
    "        H = calculate_homography_with_rigid_transform_fallback(template_bboxes, target_bboxes)\n",
    "\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)    \n",
    "    warped_image = warp_target_image(target_image, H, template_image_size)    \n",
    "    return warped_image\n",
    "\n",
    "def get_bbox_corners(bbox):\n",
    "    \"\"\"Returns the four corners of the bounding box.\"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    return [\n",
    "        (x, y),               # Top-left\n",
    "        (x + w, y),           # Top-right\n",
    "        (x, y + h),           # Bottom-left\n",
    "        (x + w, y + h)        # Bottom-right\n",
    "    ]\n",
    "\n",
    "def align_images_using_homography_corners(base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Align the target image to the base image using homography based on matched bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The base image (template).\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'bbox' key).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The warped target image.\n",
    "    \"\"\"\n",
    "    # Extract the bounding boxes from the masks and match results\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    \n",
    "    print(f\"Building homograph from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "\n",
    "    # Use the corners of the bounding boxes instead of just the center\n",
    "    template_points = []\n",
    "    target_points = []\n",
    "\n",
    "    for template_bbox, target_bbox in zip(template_bboxes, target_bboxes):\n",
    "        template_points.extend(get_bbox_corners(template_bbox))\n",
    "        target_points.extend(get_bbox_corners(target_bbox))\n",
    "        \n",
    "    # Convert points to numpy arrays with the correct shape for cv2.findHomography\n",
    "    template_points = np.array(template_points, dtype=np.float32)\n",
    "    target_points = np.array(target_points, dtype=np.float32)        \n",
    "\n",
    "    # Calculate the homography matrix\n",
    "    if len(template_points) < 4:\n",
    "        print(f\"Not enough matches to warp\")\n",
    "        return None\n",
    "\n",
    "    # H = calculate_homography(template_points, target_points)\n",
    "    H, status = cv2.findHomography(target_points, template_points, cv2.RANSAC)\n",
    "\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Warp the target image to align with the base image\n",
    "    warped_image = warp_target_image(target_image, H, template_image_size)\n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "def mask_area_filter(image,masks,min_surf=min_segment_surface, max_surf=max_segment_surface):\n",
    "    surface=image.shape[0]*image.shape[(1)]\n",
    "    return [m for m in masks if ((m['area'] / surface) > min_surf and (m['area'] / surface) < max_surf)]\n",
    "\n",
    "def match_segments(image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(image, filtered_masks, target_image)\n",
    "    return results,filtered_masks\n",
    "\n",
    "def match_and_plot_segments(base_image_name,target_image_name, image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(image, filtered_masks, target_image)\n",
    "    plot_matches_side_by_side(base_image_name,target_image_name, image, target_image, results, filtered_masks)\n",
    "    return results,filtered_masks\n",
    "\n",
    "def match_warp_and_plot_segments_using_homography(base_image_name, target_image_name, base_image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "    if(len(results) > 3):\n",
    "        warped_image = align_images_using_homography(base_image, target_image, results, filtered_masks)\n",
    "    else:\n",
    "        print(f'Only {len(results)} matches. Using bboxes in stead of centers')\n",
    "        warped_image = align_images_using_homography_corners(base_image, target_image, results, filtered_masks)\n",
    "    if(warped_image is not None):\n",
    "        plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, results, filtered_masks)\n",
    "    return results,filtered_masks,warped_image\n",
    "\n",
    "def match_warp_and_plot_segments_using_homography_with_rigid_transform_fallback(base_image_name, target_image_name, base_image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "    if(len(results) > 0):\n",
    "        warped_image = align_images_using_homography_with_rigid_transform_fallback(base_image, target_image, results, filtered_masks)\n",
    "    else:\n",
    "        print(f'ERROR! No matches. Cannot align')\n",
    "        return None,filtered_masks,None\n",
    "    if(warped_image is not None):\n",
    "        plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, results, filtered_masks)\n",
    "    else: \n",
    "        plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, None, results, filtered_masks)\n",
    "    return results,filtered_masks,warped_image\n",
    "\n",
    "def match_warp_and_plot_segments_using_rigid_transform(base_image_name, target_image_name, base_image,masks,target_image,min_segment_area=min_segment_surface,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "    if(len(results) > 3):\n",
    "        warped_image = align_images_using_rigid_transform(base_image, target_image, results, filtered_masks)\n",
    "    elif(len(results) > 0):\n",
    "        print(f'Oops-Only {len(results)} matches. Use region corners')\n",
    "        warped_image = align_images_using_homography_corners(base_image, target_image, results, filtered_masks)\n",
    "        plot_matches_side_by_side(base_image_name, target_image_name, base_image, target_image, results, masks)\n",
    "        return results,filtered_masks,warped_image\n",
    "    else:\n",
    "        print(f'ERROR {len(results)} matches. Cannot align')\n",
    "        return None,filtered_masks,None\n",
    "        \n",
    "    if(warped_image is not None):\n",
    "        plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, results, filtered_masks)  \n",
    "    return results,filtered_masks,warped_image\n",
    "\n",
    "def create_stop_motion_movie(input_folder, output_file, frame_duration=2, transition_duration=1, fps=30):\n",
    "    # Get all jpg files in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()  # Sort the files to ensure correct order\n",
    "    print(f\"files {image_files}\")\n",
    "\n",
    "    # Get the dimensions of the first image\n",
    "    first_image = cv2.imread(os.path.join(input_folder, image_files[0]))\n",
    "    height, width = first_image.shape[:2]\n",
    "\n",
    "    # Create a temporary video file\n",
    "    temp_output = 'temp_output.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))\n",
    "\n",
    "    for i in range(len(image_files)):\n",
    "        current_img = cv2.imread(os.path.join(input_folder, image_files[i]))\n",
    "        next_img = cv2.imread(os.path.join(input_folder, image_files[(i + 1) % len(image_files)]))\n",
    "        \n",
    "        # Hold the current image\n",
    "        for _ in range(fps * frame_duration):\n",
    "            out.write(current_img)\n",
    "        \n",
    "        # Cross-fade to the next image\n",
    "        for j in range(int(fps * transition_duration)):\n",
    "            alpha = j / (fps * transition_duration)\n",
    "            blended = cv2.addWeighted(current_img, 1 - alpha, next_img, alpha, 0)\n",
    "            out.write(blended)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    # Use FFmpeg to convert the temporary video to the final output with improved compression\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', temp_output,\n",
    "        '-c:v', 'libx264',\n",
    "        '-preset', 'slow',\n",
    "        '-crf', '23',\n",
    "        '-vf', f'scale=-2:720',  # Scale to 720p, maintaining aspect ratio\n",
    "        '-movflags', '+faststart',\n",
    "        '-c:a', 'aac',\n",
    "        '-b:a', '128k',\n",
    "        output_file\n",
    "    ]\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_output)\n",
    "\n",
    "    print(f\"Stop motion movie created: {output_file}\")\n",
    "\n",
    "def plot_overlayed(image,masks):\n",
    "    filtered_masks = mask_area_filter(image,masks,0.0,1.0)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    show_anns(filtered_masks)\n",
    "    plt.axis('off')\n",
    "    plt.show() \n",
    "    \n",
    "def plot_overlayed_blank_nonmatched(image, masks):\n",
    "    filtered_masks = mask_area_filter(image, masks, 0.0, 1.0)\n",
    "    combined_mask = np.zeros(image.shape[:2], dtype=bool)\n",
    "    for mask in filtered_masks:\n",
    "        combined_mask = np.logical_or(combined_mask, mask[\"segmentation\"])\n",
    "    masked_image = np.zeros_like(image)\n",
    "    masked_image[combined_mask] = image[combined_mask]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Source Image\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(masked_image)\n",
    "    show_anns(filtered_masks)  # Assuming show_anns works with filtered_masks\n",
    "    axes[1].set_title(\"Segmented Image\")\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def crop_center(image, crop_width, crop_height):\n",
    "    \"\"\"Crops the image from the center with given width and height.\"\"\"\n",
    "    img_width, img_height = image.size\n",
    "    left = (img_width - crop_width) // 2\n",
    "    top = (img_height - crop_height) // 2\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    return image.crop((left, top, right, bottom))\n",
    "\n",
    "def add_timestamp_from_exif(orig_image_path, image):\n",
    "    # Timestamp from EXIF\n",
    "    if(os.path.exists(orig_image_path)):\n",
    "        orig_image = Image.open(orig_image_path)\n",
    "        exif_data = piexif.load(orig_image.info.get('exif', b''))\n",
    "        exif_datetime = exif_data.get('Exif', {}).get(piexif.ExifIFD.DateTimeOriginal)\n",
    "    \n",
    "    if os.path.exists(orig_image_path) and exif_datetime:\n",
    "        dt = datetime.strptime(exif_datetime.decode('utf-8'), '%Y:%m:%d %H:%M:%S')\n",
    "        timestamp_str = dt.strftime('%d-%b %H:%M')\n",
    "\n",
    "        # Convert OpenCV BGR image to RGB for PIL\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(rgb_image)\n",
    "\n",
    "        # Add timestamp using PIL\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        font_path = \"/Library/Fonts/PTSans-Regular.ttf\"\n",
    "        font_size = 40\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "        img_width, img_height = pil_image.size\n",
    "        text_position = (img_width // 2, 80)  # Top center, 12 pixels from the top\n",
    "        draw.text(text_position, timestamp_str, font=font, fill=(255, 255, 255), anchor=\"ms\")\n",
    "\n",
    "        # Convert back to OpenCV BGR format\n",
    "        result_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "        # result_image = pil_image\n",
    "        return result_image\n",
    "    else:\n",
    "        print('No EXIF datetime for the image')\n",
    "        return image\n",
    "\n",
    "def warp_target_image_with_translation(target_image, translation_vector, template_image_size):\n",
    "    \"\"\"\n",
    "    Warp the target image using the translation vector.\n",
    "    \n",
    "    Args:\n",
    "        target_image (numpy.ndarray): The target image to be warped.\n",
    "        translation_vector (tuple): The (x, y) translation vector.\n",
    "        template_image_size (tuple): The size of the template image (width, height).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The translated target image.\n",
    "    \"\"\"\n",
    "    # Create a translation matrix\n",
    "    translation_matrix = np.float32([[1, 0, translation_vector[0]], [0, 1, translation_vector[1]]])\n",
    "    \n",
    "    # Warp the target image using only translation\n",
    "    translated_image = cv2.warpAffine(target_image, translation_matrix, template_image_size)\n",
    "    \n",
    "    return translated_image\n",
    "\n",
    "def calculate_translation(template_bboxes, target_bboxes):\n",
    "    \"\"\"\n",
    "    Calculate the average translation vector between template and target bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        template_bboxes (list): List of bounding boxes from the template (base) image.\n",
    "        target_bboxes (list): List of bounding boxes from the target image.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The (x, y) translation vector.\n",
    "    \"\"\"\n",
    "    # Initialize translation deltas\n",
    "    delta_x = 0\n",
    "    delta_y = 0\n",
    "    \n",
    "    # Calculate the translation for each corresponding pair of bounding boxes\n",
    "    for template_bbox, target_bbox in zip(template_bboxes, target_bboxes):\n",
    "        template_center_x = template_bbox[0] + template_bbox[2] / 2\n",
    "        template_center_y = template_bbox[1] + template_bbox[3] / 2\n",
    "        \n",
    "        target_center_x = target_bbox[0] + target_bbox[2] / 2\n",
    "        target_center_y = target_bbox[1] + target_bbox[3] / 2\n",
    "        \n",
    "        delta_x += template_center_x - target_center_x\n",
    "        delta_y += template_center_y - target_center_y\n",
    "    \n",
    "    # Average the translation across all matched bounding boxes\n",
    "    num_matches = len(template_bboxes)\n",
    "    if num_matches == 0:\n",
    "        return (0, 0)\n",
    "    \n",
    "    avg_delta_x = delta_x / num_matches\n",
    "    avg_delta_y = delta_y / num_matches\n",
    "    \n",
    "    return (avg_delta_x, avg_delta_y)\n",
    "\n",
    "def align_images_using_translation(base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Align the target image to the base image using translation based on matched bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The base image (template).\n",
    "        target_image (numpy.ndarray): The target image to be translated.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'bbox' key).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The translated target image.\n",
    "    \"\"\"\n",
    "    # Extract the bounding boxes from the masks and match results\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    print(f\"Building translation from {len(target_bboxes)} target_bboxes\")\n",
    "    print(f\"...template:{template_bboxes}\")\n",
    "    print(f\"...target:{target_bboxes}\")\n",
    "    \n",
    "    if len(template_bboxes) == 0 or len(target_bboxes) == 0:\n",
    "        print(\"Not enough matches to perform translation\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate the average translation vector\n",
    "    translation_vector = calculate_translation(template_bboxes, target_bboxes)\n",
    "    print(f\"Calculated translation vector: {translation_vector}\")\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Warp (translate) the target image to align with the base image\n",
    "    translated_image = warp_target_image_with_translation(target_image, translation_vector, template_image_size)\n",
    "    \n",
    "    return translated_image\n",
    "\n",
    "def match_warp_and_plot_segments_using_translation(base_image_name, target_image_name, base_image,masks,target_image,min_segment_area=0.0015,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "    warped_image = None\n",
    "    warped_image = align_images_using_translation(base_image, target_image, results, filtered_masks)\n",
    "    if(warped_image is not None):\n",
    "        plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, results, filtered_masks)\n",
    "\n",
    "    return results,filtered_masks,warped_image\n",
    "\n",
    "def calculate_translation_and_rotation(template_bboxes, target_bboxes):\n",
    "    \"\"\"\n",
    "    Calculate the average translation vector and rotation angle between template and target bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        template_bboxes (list): List of bounding boxes from the template (base) image.\n",
    "        target_bboxes (list): List of bounding boxes from the target image.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The (x, y) translation vector and the average rotation angle (in degrees).\n",
    "    \"\"\"\n",
    "    delta_x = 0\n",
    "    delta_y = 0\n",
    "    delta_angle = 0\n",
    "    \n",
    "    for template_bbox, target_bbox in zip(template_bboxes, target_bboxes):\n",
    "        # Calculate the centers of the bounding boxes\n",
    "        template_center_x = template_bbox[0] + template_bbox[2] / 2\n",
    "        template_center_y = template_bbox[1] + template_bbox[3] / 2\n",
    "        \n",
    "        target_center_x = target_bbox[0] + target_bbox[2] / 2\n",
    "        target_center_y = target_bbox[1] + target_bbox[3] / 2\n",
    "        \n",
    "        delta_x += template_center_x - target_center_x\n",
    "        delta_y += template_center_y - target_center_y\n",
    "        \n",
    "        # Calculate the angle between the width/height ratios of the bounding boxes\n",
    "        template_ratio = np.arctan2(template_bbox[3], template_bbox[2])\n",
    "        target_ratio = np.arctan2(target_bbox[3], target_bbox[2])\n",
    "        \n",
    "        delta_angle += np.rad2deg(template_ratio - target_ratio)\n",
    "    \n",
    "    num_matches = len(template_bboxes)\n",
    "    if num_matches == 0:\n",
    "        return (0, 0, 0)\n",
    "    \n",
    "    avg_delta_x = delta_x / num_matches\n",
    "    avg_delta_y = delta_y / num_matches\n",
    "    avg_delta_angle = delta_angle / num_matches\n",
    "    \n",
    "    return (avg_delta_x, avg_delta_y, avg_delta_angle)\n",
    "\n",
    "def align_images_using_translation_and_rotation(base_image, target_image, match_results, masks):\n",
    "    \"\"\"\n",
    "    Align the target image to the base image using translation and rotation based on matched bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        base_image (numpy.ndarray): The base image (template).\n",
    "        target_image (numpy.ndarray): The target image to be transformed.\n",
    "        match_results (list): List of dictionaries containing match information for each mask.\n",
    "        masks (list): List of SAM-generated masks (with 'bbox' key).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed target image.\n",
    "    \"\"\"\n",
    "    template_bboxes = [masks[result['mask_index']]['bbox'] for result in match_results]\n",
    "    target_bboxes = [(result['best_match_loc'][0], result['best_match_loc'][1], result['segment_shape'][1], result['segment_shape'][0]) for result in match_results]\n",
    "    \n",
    "    if len(template_bboxes) == 0 or len(target_bboxes) == 0:\n",
    "        print(\"Not enough matches to perform transformation\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate the average translation vector and rotation angle\n",
    "    avg_delta_x, avg_delta_y, avg_delta_angle = calculate_translation_and_rotation(template_bboxes, target_bboxes)\n",
    "    translation_vector = (avg_delta_x, avg_delta_y)\n",
    "    \n",
    "    print(f\"Calculated translation vector: {translation_vector}, Rotation angle: {avg_delta_angle}\")\n",
    "    \n",
    "    # Get the size of the base (template) image\n",
    "    template_image_size = (base_image.shape[1], base_image.shape[0])  # (width, height)\n",
    "    \n",
    "    # Create a transformation matrix for both translation and rotation\n",
    "    center = (template_image_size[0] // 2, template_image_size[1] // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, avg_delta_angle, 1.0)\n",
    "    \n",
    "    # Apply the translation to the transformation matrix\n",
    "    rotation_matrix[0, 2] += translation_vector[0]\n",
    "    rotation_matrix[1, 2] += translation_vector[1]\n",
    "    \n",
    "    # Warp (transform) the target image\n",
    "    transformed_image = cv2.warpAffine(target_image, rotation_matrix, template_image_size)\n",
    "    \n",
    "    return transformed_image\n",
    "\n",
    "def match_warp_and_plot_segments_using_translation_and_rotation(base_image_name, target_image_name, base_image,masks,target_image,min_segment_area=0.0015,max_segment_area=max_segment_surface):\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "    results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "    warped_image = None    \n",
    "    if(len(results) > 3):\n",
    "        warped_image = align_images_using_translation_and_rotation(base_image, target_image, results, filtered_masks)\n",
    "        if(warped_image is not None):\n",
    "            plot_matches_and_warped_side_by_side(base_image_name, target_image_name, base_image, target_image, warped_image, results, filtered_masks)\n",
    "    return results,filtered_masks,warped_image\n",
    "\n",
    "def get_or_generate_masks(mask_generator: Any, image_path: str, saved_masks_folder: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves saved masks if they exist; otherwise, generates masks using the provided generator,\n",
    "    saves them, and returns the generated masks.\n",
    "\n",
    "    Args:\n",
    "        mask_generator (Any): The SAM2 mask generator instance.\n",
    "        image_path (str): Path to the input image.\n",
    "        saved_masks_folder (str): Path to the folder where masks are saved/loaded.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of masks for the image.\n",
    "    \"\"\"\n",
    "    # Ensure the saved masks folder exists\n",
    "    os.makedirs(saved_masks_folder, exist_ok=True)\n",
    "\n",
    "    # Derive a unique filename for the mask based on the image path\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    mask_filename = os.path.splitext(image_filename)[0] + \"_masks.pkl\"\n",
    "    mask_filepath = os.path.join(saved_masks_folder, mask_filename)\n",
    "\n",
    "    # Check if the mask file exists\n",
    "    if os.path.exists(mask_filepath):\n",
    "        # Load and return the saved masks\n",
    "        with open(mask_filepath, 'rb') as file:\n",
    "            masks = pickle.load(file)\n",
    "        print(f\"Loaded masks from {mask_filepath}\")\n",
    "        return masks\n",
    "\n",
    "    # If masks are not found, generate them\n",
    "    print(f\"Masks not found. Generating new masks for {image_path}...\")\n",
    "    import cv2\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "    # Generate masks using the provided mask generator\n",
    "    masks = mask_generator.generate(image)\n",
    "\n",
    "    # Save the generated masks\n",
    "    with open(mask_filepath, 'wb') as file:\n",
    "        pickle.dump(masks, file)\n",
    "    print(f\"Generated masks saved to {mask_filepath}\")\n",
    "\n",
    "    return masks\n",
    "\n",
    "def load_image(image_path: str) -> Any:\n",
    "    \"\"\"Load an image from the given path.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def process_images_and_masks(\n",
    "    folder_path: str, \n",
    "    saved_masks_folder: str, \n",
    "    mask_generator: Any\n",
    ") -> Tuple[List[Any], List[List[dict]]]:\n",
    "    \"\"\"\n",
    "    Processes all images in the specified folder, generates or retrieves masks for each,\n",
    "    and returns a list of images and their corresponding masks.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        saved_masks_folder (str): Path to the folder where masks are saved/loaded.\n",
    "        mask_generator (Any): The SAM2 mask generator instance.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Any], List[List[dict]]]: A tuple of lists containing images and their masks.\n",
    "    \"\"\"\n",
    "    # Ensure the saved masks folder exists\n",
    "    os.makedirs(saved_masks_folder, exist_ok=True)\n",
    "\n",
    "    # Sort the list of image files in the folder\n",
    "    image_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.jpg')])\n",
    "\n",
    "    # Load all images and generate/retrieve their masks\n",
    "    images_list = []\n",
    "    masks_list = []\n",
    "\n",
    "    for file in image_files:\n",
    "        image_path = os.path.join(folder_path, file)\n",
    "        # Load image\n",
    "        image = load_image(image_path)\n",
    "        images_list.append(image)\n",
    "\n",
    "        # Generate or retrieve masks\n",
    "        masks = get_or_generate_masks(mask_generator, image_path, saved_masks_folder)\n",
    "        masks_list.append(masks)\n",
    "\n",
    "    return image_files,images_list, masks_list\n",
    "\n",
    "def calculate_homography_and_display(image1, image2, masks1, masks2):\n",
    "    \"\"\"\n",
    "    Calculates the homography based on IoU of SAM masks, applies the homography, \n",
    "    and displays the first image, second image, and the warped image side by side.\n",
    "\n",
    "    Args:\n",
    "        image1 (np.ndarray): First image (reference).\n",
    "        image2 (np.ndarray): Second image to align.\n",
    "        masks1 (List[dict]): Masks from SAM for the first image.\n",
    "        masks2 (List[dict]): Masks from SAM for the second image.\n",
    "    \"\"\"\n",
    "    def compute_iou(mask1, mask2):\n",
    "        \"\"\"Computes IoU between two binary masks.\"\"\"\n",
    "        intersection = np.logical_and(mask1, mask2).sum()\n",
    "        union = np.logical_or(mask1, mask2).sum()\n",
    "        return intersection / union\n",
    "\n",
    "    # Match masks based on IoU\n",
    "    matched_points = []\n",
    "    for mask1 in masks1:\n",
    "        seg1 = mask1[\"segmentation\"]\n",
    "        best_match = None\n",
    "        best_iou = 0\n",
    "        for mask2 in masks2:\n",
    "            seg2 = mask2[\"segmentation\"]\n",
    "            iou = compute_iou(seg1, seg2)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = mask2\n",
    "        \n",
    "        if best_match and best_iou > 0.3:  # Keep only meaningful matches\n",
    "            # Use bounding box centers as keypoints\n",
    "            bbox1 = mask1[\"bbox\"]\n",
    "            bbox2 = best_match[\"bbox\"]\n",
    "            center1 = (bbox1[0] + bbox1[2] / 2, bbox1[1] + bbox1[3] / 2)\n",
    "            center2 = (bbox2[0] + bbox2[2] / 2, bbox2[1] + bbox2[3] / 2)\n",
    "            matched_points.append((center1, center2))\n",
    "\n",
    "    # Check if we have enough points to calculate homography\n",
    "    if len(matched_points) >= 4:\n",
    "        # Separate matched points into source and destination\n",
    "        src_points = np.array([p[0] for p in matched_points], dtype=np.float32)\n",
    "        dst_points = np.array([p[1] for p in matched_points], dtype=np.float32)\n",
    "\n",
    "        # Calculate the homography matrix\n",
    "        H, status = cv2.findHomography(dst_points, src_points, cv2.RANSAC)\n",
    "\n",
    "        # Warp the second image to align with the first\n",
    "        warped_image = cv2.warpPerspective(image2, H, (image1.shape[1], image1.shape[0]))\n",
    "        # Display the images side by side\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"First Image (Reference)\")\n",
    "        plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Second Image (To Align)\")\n",
    "        plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Warped Image\")\n",
    "        plt.imshow(cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"Not enough matched points to calculate homography.\")\n",
    "        # Display the images side by side\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"First Image (Reference)\")\n",
    "        plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Second Image (To Align)\")\n",
    "        plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def calculate_homography_with_translation(image1, image2, masks1, masks2):\n",
    "    \"\"\"\n",
    "    Calculates the homography by first estimating translation of SAM masks\n",
    "    and then refining using feature matching. Displays the results.\n",
    "\n",
    "    Args:\n",
    "        image1 (np.ndarray): First image (reference).\n",
    "        image2 (np.ndarray): Second image to align.\n",
    "        masks1 (List[dict]): Masks from SAM for the first image.\n",
    "        masks2 (List[dict]): Masks from SAM for the second image.\n",
    "    \"\"\"\n",
    "    def compute_iou(mask1, mask2):\n",
    "        \"\"\"Computes IoU between two binary masks.\"\"\"\n",
    "        intersection = np.logical_and(mask1, mask2).sum()\n",
    "        union = np.logical_or(mask1, mask2).sum()\n",
    "        return intersection / union\n",
    "\n",
    "    # Step 1: Coarse Matching Based on IoU\n",
    "    matched_masks = []\n",
    "    for mask1 in masks1:\n",
    "        seg1 = mask1[\"segmentation\"]\n",
    "        best_match = None\n",
    "        best_iou = 0\n",
    "        for mask2 in masks2:\n",
    "            seg2 = mask2[\"segmentation\"]\n",
    "            iou = compute_iou(seg1, seg2)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = mask2\n",
    "        \n",
    "        if best_match and best_iou > 0.3:  # Lower IoU threshold\n",
    "            matched_masks.append((mask1, best_match))\n",
    "\n",
    "    if not matched_masks:\n",
    "        raise ValueError(\"No sufficiently matching masks found between the images.\")\n",
    "\n",
    "    # Step 2: Estimate Rough Translation\n",
    "    src_points = []\n",
    "    dst_points = []\n",
    "    for mask1, mask2 in matched_masks:\n",
    "        bbox1 = mask1[\"bbox\"]\n",
    "        bbox2 = mask2[\"bbox\"]\n",
    "\n",
    "        # Calculate bounding box centers\n",
    "        center1 = (bbox1[0] + bbox1[2] / 2, bbox1[1] + bbox1[3] / 2)\n",
    "        center2 = (bbox2[0] + bbox2[2] / 2, bbox2[1] + bbox2[3] / 2)\n",
    "\n",
    "        src_points.append(center1)\n",
    "        dst_points.append(center2)\n",
    "\n",
    "    src_points = np.array(src_points, dtype=np.float32)\n",
    "    dst_points = np.array(dst_points, dtype=np.float32)\n",
    "\n",
    "    # Compute rough translation matrix\n",
    "    translation_matrix, _ = cv2.estimateAffinePartial2D(dst_points, src_points)\n",
    "    translated_image2 = cv2.warpAffine(image2, translation_matrix, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "    # Step 3: Feature Matching for Refinement\n",
    "    orb = cv2.ORB_create(nfeatures=500)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(image1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(translated_image2, None)\n",
    "\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        raise ValueError(\"Feature descriptors could not be computed for one of the images.\")\n",
    "\n",
    "    # Match features\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Map matched keypoints to original image coordinates\n",
    "    matched_src_points = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "    matched_dst_points = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    # Compute final homography\n",
    "    H, status = cv2.findHomography(matched_dst_points, matched_src_points, cv2.RANSAC)\n",
    "\n",
    "    # Warp image2 to align with image1\n",
    "    warped_image = cv2.warpPerspective(image2, H, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "    # Step 4: Display Results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"First Image (Reference)\")\n",
    "    plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Second Image (To Align)\")\n",
    "    plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Warped Image\")\n",
    "    plt.imshow(cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_shape_distance_based_matching(image1, image2, masks1, masks2, distance_weight=0.5):\n",
    "    \"\"\"\n",
    "    Matches SAM segments between two images based on shape similarity and spatial proximity.\n",
    "    Calculates and applies the homography.\n",
    "\n",
    "    Args:\n",
    "        image1 (np.ndarray): First image (reference).\n",
    "        image2 (np.ndarray): Second image to align.\n",
    "        masks1 (List[dict]): Masks from SAM for the first image.\n",
    "        masks2 (List[dict]): Masks from SAM for the second image.\n",
    "        distance_weight (float): Weight for distance penalty in the matching score.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The warped version of image2.\n",
    "    \"\"\"\n",
    "    def compute_hu_similarity(seg1, seg2):\n",
    "        \"\"\"Compute shape similarity using Hu Moments.\"\"\"\n",
    "        moments1 = cv2.moments(seg1.astype(np.uint8))\n",
    "        moments2 = cv2.moments(seg2.astype(np.uint8))\n",
    "        hu1 = cv2.HuMoments(moments1).flatten()\n",
    "        hu2 = cv2.HuMoments(moments2).flatten()\n",
    "        # Log-transform for better comparison\n",
    "        hu1 = -np.sign(hu1) * np.log10(np.abs(hu1) + 1e-8)\n",
    "        hu2 = -np.sign(hu2) * np.log10(np.abs(hu2) + 1e-8)\n",
    "        return 1.0 / (1.0 + np.linalg.norm(hu1 - hu2))  # Higher is better\n",
    "\n",
    "    def compute_distance_penalty(center1, center2):\n",
    "        \"\"\"Compute penalty based on Euclidean distance.\"\"\"\n",
    "        distance = np.linalg.norm(np.array(center1) - np.array(center2))\n",
    "        return 1.0 / (1.0 + distance)  # Higher is better\n",
    "\n",
    "    # Match masks with combined shape and distance score\n",
    "    matched_points = []\n",
    "    for mask1 in masks1:\n",
    "        seg1 = mask1[\"segmentation\"]\n",
    "        center1 = (\n",
    "            mask1[\"bbox\"][0] + mask1[\"bbox\"][2] / 2,\n",
    "            mask1[\"bbox\"][1] + mask1[\"bbox\"][3] / 2,\n",
    "        )\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        for mask2 in masks2:\n",
    "            seg2 = mask2[\"segmentation\"]\n",
    "            center2 = (\n",
    "                mask2[\"bbox\"][0] + mask2[\"bbox\"][2] / 2,\n",
    "                mask2[\"bbox\"][1] + mask2[\"bbox\"][3] / 2,\n",
    "            )\n",
    "            shape_similarity = compute_hu_similarity(seg1, seg2)\n",
    "            distance_penalty = compute_distance_penalty(center1, center2)\n",
    "            score = shape_similarity * (1 - distance_weight) + distance_penalty * distance_weight\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = center2\n",
    "\n",
    "        if best_match:\n",
    "            matched_points.append((center1, best_match))\n",
    "\n",
    "    if len(matched_points) < 4:\n",
    "        raise ValueError(\"Not enough matched points to calculate homography.\")\n",
    "\n",
    "    # Compute homography\n",
    "    src_points = np.array([p[0] for p in matched_points], dtype=np.float32)\n",
    "    dst_points = np.array([p[1] for p in matched_points], dtype=np.float32)\n",
    "    H, _ = cv2.findHomography(dst_points, src_points, cv2.RANSAC)\n",
    "\n",
    "    # Warp the second image\n",
    "    warped_image = cv2.warpPerspective(image2, H, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "    # Display the images side by side\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"First Image (Reference)\")\n",
    "    plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Second Image (To Align)\")\n",
    "    plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Warped Image\")\n",
    "    plt.imshow(cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return warped_image\n",
    "\n",
    "def calculate_and_display_best_matches(image1, image2, masks1, masks2, distance_weight=0.5, top_k=6):\n",
    "    \"\"\"\n",
    "    Matches SAM segments between two images based on shape similarity and spatial proximity,\n",
    "    calculates and applies the homography, and displays the best matches.\n",
    "\n",
    "    Args:\n",
    "        image1 (np.ndarray): First image (reference).\n",
    "        image2 (np.ndarray): Second image to align.\n",
    "        masks1 (List[dict]): Masks from SAM for the first image.\n",
    "        masks2 (List[dict]): Masks from SAM for the second image.\n",
    "        distance_weight (float): Weight for distance penalty in the matching score.\n",
    "        top_k (int): Number of best matches to visualize.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The warped version of image2.\n",
    "    \"\"\"\n",
    "    def compute_hu_similarity(seg1, seg2):\n",
    "        \"\"\"Compute shape similarity using Hu Moments.\"\"\"\n",
    "        moments1 = cv2.moments(seg1.astype(np.uint8))\n",
    "        moments2 = cv2.moments(seg2.astype(np.uint8))\n",
    "        hu1 = cv2.HuMoments(moments1).flatten()\n",
    "        hu2 = cv2.HuMoments(moments2).flatten()\n",
    "        hu1 = -np.sign(hu1) * np.log10(np.abs(hu1) + 1e-8)\n",
    "        hu2 = -np.sign(hu2) * np.log10(np.abs(hu2) + 1e-8)\n",
    "        return 1.0 / (1.0 + np.linalg.norm(hu1 - hu2))\n",
    "\n",
    "    def compute_distance_penalty(center1, center2):\n",
    "        \"\"\"Compute penalty based on Euclidean distance.\"\"\"\n",
    "        distance = np.linalg.norm(np.array(center1) - np.array(center2))\n",
    "        return 1.0 / (1.0 + distance)\n",
    "\n",
    "    # Step 1: Match masks with combined shape and distance score\n",
    "    match_scores = []\n",
    "    for mask1 in masks1:\n",
    "        seg1 = mask1[\"segmentation\"]\n",
    "        center1 = (\n",
    "            mask1[\"bbox\"][0] + mask1[\"bbox\"][2] / 2,\n",
    "            mask1[\"bbox\"][1] + mask1[\"bbox\"][3] / 2,\n",
    "        )\n",
    "        for mask2 in masks2:\n",
    "            seg2 = mask2[\"segmentation\"]\n",
    "            center2 = (\n",
    "                mask2[\"bbox\"][0] + mask2[\"bbox\"][2] / 2,\n",
    "                mask2[\"bbox\"][1] + mask2[\"bbox\"][3] / 2,\n",
    "            )\n",
    "            shape_similarity = compute_hu_similarity(seg1, seg2)\n",
    "            distance_penalty = compute_distance_penalty(center1, center2)\n",
    "            score = shape_similarity * (1 - distance_weight) + distance_penalty * distance_weight\n",
    "            match_scores.append((score, mask1, mask2, center1, center2))\n",
    "\n",
    "    # Step 2: Select top-k matches\n",
    "    match_scores = sorted(match_scores, key=lambda x: -x[0])[:top_k]\n",
    "    matched_points = [(m[3], m[4]) for m in match_scores]\n",
    "\n",
    "    if len(matched_points) < 4:\n",
    "        raise ValueError(\"Not enough matched points to calculate homography.\")\n",
    "\n",
    "    # Step 3: Compute homography\n",
    "    src_points = np.array([p[0] for p in matched_points], dtype=np.float32)\n",
    "    dst_points = np.array([p[1] for p in matched_points], dtype=np.float32)\n",
    "    H, _ = cv2.findHomography(dst_points, src_points, cv2.RANSAC)\n",
    "\n",
    "    # Warp the second image\n",
    "    warped_image = cv2.warpPerspective(image2, H, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "    # Step 4: Visualize Matches\n",
    "    image1_copy = image1.copy()\n",
    "    image2_copy = image2.copy()\n",
    "    image2_offset = image1.shape[1]  # Offset for second image in the combined image\n",
    "\n",
    "    combined_image = np.concatenate((image1_copy, image2_copy), axis=1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    for i, (score, mask1, mask2, center1, center2) in enumerate(match_scores):\n",
    "        # Draw segments\n",
    "        color = tuple(np.random.randint(0, 255, size=(3,), dtype=int))  # Random color\n",
    "\n",
    "        # Color segmentation in the first image\n",
    "        seg1 = mask1[\"segmentation\"]\n",
    "        image1_copy[seg1] = color\n",
    "\n",
    "        # Color segmentation in the second image\n",
    "        seg2 = mask2[\"segmentation\"]\n",
    "        image2_copy[seg2] = color\n",
    "\n",
    "        # Draw lines connecting matched points\n",
    "        pt1_x, pt1_y = int(center1[0]), int(center1[1])\n",
    "        pt2_x, pt2_y = int(center2[0] + image2_offset), int(center2[1])  # Adjust for offset\n",
    "\n",
    "        plt.scatter(pt1_x, pt1_y, color='red', s=50, label=\"Image1 Match\" if i == 0 else \"\")\n",
    "        plt.scatter(pt2_x, pt2_y, color='blue', s=50, label=\"Image2 Match\" if i == 0 else \"\")\n",
    "        plt.plot([pt1_x, pt2_x], [pt1_y, pt2_y], color='yellow', linewidth=1)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return warped_image\n",
    "\n",
    "def plot_images_with_masks(image_files,images,masks):\n",
    "    base_image_ix = image_files.index(base_image)\n",
    "    for idx,img in enumerate(images):\n",
    "        print(idx,image_files[idx],len(masks[idx]))\n",
    "        plot_overlayed_blank_nonmatched(img,masks[idx])\n",
    "\n",
    "def process_and_display_homographies(images_list, masks_list):\n",
    "    \"\"\"\n",
    "    Calculates homographies for a series of images relative to the first image,\n",
    "    applies the transformations, and displays the results.\n",
    "\n",
    "    Args:\n",
    "        images_list (List[np.ndarray]): List of images to process.\n",
    "        masks_list (List[List[dict]]): List of SAM masks corresponding to the images.\n",
    "    \"\"\"\n",
    "    # Use the first image and its masks as the base\n",
    "    base_image = images_list[0]\n",
    "    base_masks = masks_list[0]\n",
    "\n",
    "    # Iterate through the rest of the images\n",
    "    for idx, (image, masks) in enumerate(zip(images_list[1:], masks_list[1:]), start=1):\n",
    "        print(f\"Processing image {idx + 1}/{len(images_list)}...\")\n",
    "\n",
    "        try:\n",
    "            # Calculate homography and display the results\n",
    "            # calculate_homography_and_display(base_image, image, base_masks, masks)\n",
    "            # calculate_homography_with_translation(base_image, image, base_masks, masks)\n",
    "            # calculate_shape_distance_based_matching(base_image, image, base_masks, masks)\n",
    "            # calculate_and_display_matches_with_segments(base_image, image, base_masks, masks)\n",
    "            calculate_and_display_best_matches(base_image, image, base_masks, masks)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping image {idx + 1}: {e}\")\n",
    "\n",
    "def get_mask_by_image_name(image_name, image_files, masks_list):\n",
    "    \"\"\"\n",
    "    Retrieve the mask corresponding to a given image file name.\n",
    "\n",
    "    :param image_name: The name of the image file to search for.\n",
    "    :param image_files: List of image file names.\n",
    "    :param masks_list: List of masks corresponding to the image files.\n",
    "    :return: The mask corresponding to the given image file name, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the index of the image in the list\n",
    "        index = image_files.index(image_name)\n",
    "        # Return the corresponding mask\n",
    "        return masks_list[index]\n",
    "    except ValueError:\n",
    "        # If the image file is not found, return None\n",
    "        print(f\"Image file '{image_name}' not found in the list.\")\n",
    "        return None            \n",
    "\n",
    "def plot_in_grid_with_titles(images_list, names_list, cols=4, figsize=(15, 5)):\n",
    "    if len(images_list) != len(names_list):\n",
    "        raise ValueError(\"The length of images_list and names_list must be the same.\")\n",
    "\n",
    "    num_images = len(images_list)\n",
    "    num_rows = (num_images + cols - 1) // cols  # Calculate the required number of rows\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, cols, figsize=figsize)\n",
    "    axes = axes.ravel()  # Flatten the axes for easy indexing\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        if i < num_images:\n",
    "            axes[i].imshow(images_list[i])\n",
    "            axes[i].set_title(names_list[i], fontsize=10)  # Add title to the image\n",
    "            axes[i].axis('off')  # Hide axes for better visualization\n",
    "        else:\n",
    "            axes[i].axis('off')  # Hide any extra empty subplot spaces\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def align_and_plot_images(image_files,base_image_ix, images_list, masks, min_segment_area=0.0015, max_segment_area=1.0):\n",
    "    \"\"\"\n",
    "    Aligns images in the list to the base image using translation, rotation, and homography.\n",
    "    Displays the base image with bounding boxes of filtered masks, the target image with bounding boxes\n",
    "    of match results, and the aligned images in a single row for each target image.\n",
    "\n",
    "    \"\"\"\n",
    "    base_image = images_list[base_image_ix]\n",
    "    # Filter masks based on area thresholds\n",
    "    filtered_masks = mask_area_filter(base_image, masks, min_segment_area, max_segment_area)\n",
    "\n",
    "    for target_image_ix, target_image in enumerate(images_list):\n",
    "        # Match segments between the base image and target image\n",
    "        match_results = process_all_masks(base_image, filtered_masks, target_image)\n",
    "\n",
    "        # Align using translation\n",
    "        translated_image = align_images_using_translation(base_image, target_image, match_results, filtered_masks)\n",
    "\n",
    "        # Align using translation and rotation\n",
    "        translated_rotated_image = align_images_using_translation_and_rotation(base_image, target_image, match_results, filtered_masks)\n",
    "\n",
    "        # Align using homography\n",
    "        if(len(match_results) > 3):\n",
    "            homography_aligned_image = align_images_using_homography(base_image, target_image, match_results, filtered_masks)\n",
    "        else:\n",
    "            print(f'Only {len(match_results)} matches. Using bboxes in stead of centers')\n",
    "            homography_aligned_image = align_images_using_homography_corners(base_image, target_image, match_results, filtered_masks)\n",
    "\n",
    "        # Create copies for drawing bounding boxes\n",
    "        base_with_masks = base_image.copy()\n",
    "        target_with_matches = target_image.copy()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "        titles = [\n",
    "            \"Base Image with Filtered Masks\",\n",
    "            \"Target Image with Matches\",\n",
    "            \"Translated\",\n",
    "            \"Translated + Rotated\",\n",
    "            \"Homography Aligned\"\n",
    "        ]\n",
    "\n",
    "        # Plot base image with bounding boxes for filtered masks\n",
    "        axes[0].imshow(base_with_masks)\n",
    "        axes[0].set_title(image_files[base_image_ix], fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        for idx, mr in enumerate(match_results):\n",
    "            mask=filtered_masks[mr['mask_index']]\n",
    "            base_bbox = mask['bbox']\n",
    "            rect = patches.Rectangle(\n",
    "                (base_bbox[0], base_bbox[1]),\n",
    "                base_bbox[2],\n",
    "                base_bbox[3],\n",
    "                linewidth=2,\n",
    "                edgecolor='blue',\n",
    "                facecolor='none'\n",
    "            )\n",
    "            axes[0].add_patch(rect)\n",
    "            axes[0].text(\n",
    "                base_bbox[0] + base_bbox[2] / 2,\n",
    "                base_bbox[1] + base_bbox[3] / 2,\n",
    "                str(idx),\n",
    "                color='white',\n",
    "                fontsize=10,\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                bbox=dict(facecolor='blue', edgecolor='none', alpha=0.5)\n",
    "            )\n",
    "\n",
    "        # Plot target image with bounding boxes for match results\n",
    "        axes[1].imshow(target_with_matches)\n",
    "        axes[1].set_title(image_files[target_image_ix], fontsize=10)\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        for idx, match in enumerate(match_results):\n",
    "            target_bbox = (\n",
    "                match['best_match_loc'][0],\n",
    "                match['best_match_loc'][1],\n",
    "                match['segment_shape'][1],\n",
    "                match['segment_shape'][0]\n",
    "            )\n",
    "            rect = patches.Rectangle(\n",
    "                (target_bbox[0], target_bbox[1]),\n",
    "                target_bbox[2],\n",
    "                target_bbox[3],\n",
    "                linewidth=2,\n",
    "                edgecolor='green',\n",
    "                facecolor='none'\n",
    "            )\n",
    "            axes[1].add_patch(rect)\n",
    "            axes[1].text(\n",
    "                target_bbox[0] + target_bbox[2] / 2,\n",
    "                target_bbox[1] + target_bbox[3] / 2,\n",
    "                str(idx),\n",
    "                color='white',\n",
    "                fontsize=10,\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                bbox=dict(facecolor='green', edgecolor='none', alpha=0.5)\n",
    "            )\n",
    "\n",
    "        # Plot the aligned images\n",
    "        axes[2].imshow(translated_image)\n",
    "        axes[2].set_title(titles[2], fontsize=10)\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        axes[3].imshow(translated_rotated_image)\n",
    "        axes[3].set_title(titles[3], fontsize=10)\n",
    "        axes[3].axis('off')\n",
    "\n",
    "        axes[4].imshow(homography_aligned_image)\n",
    "        axes[4].set_title(titles[4], fontsize=10)\n",
    "        axes[4].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "import re\n",
    "def save_crop_exif(src_image_path,warped_image, crop_width, crop_height, warped_images_folder):\n",
    "    rgb_image = cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_image)    \n",
    "    if(crop):\n",
    "        cropped_img = crop_center(pil_image, crop_width, crop_height)\n",
    "    else:\n",
    "        cropped_img = pil_image\n",
    "    opencv_cropped_img = np.array(cropped_img)\n",
    "    opencv_cropped_img_bgr = opencv_cropped_img\n",
    "    src_image_path = re.sub(r\"_reference\", \"\", src_image_path)\n",
    "    final_img = add_timestamp_from_exif(f\"{src_image_path}\", opencv_cropped_img_bgr)\n",
    "    final_img = Image.fromarray(np.array(final_img))\n",
    "    final_img.save(f'{warped_images_folder}/{os.path.basename(src_image_path)}')\n",
    "\n",
    "def convert_pedometer_file(input_file):\n",
    "    \"\"\"\n",
    "    Converts the Pedometer_Backup.txt file into a DataFrame with three columns: date, daysteps, and steps.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input text file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns date, daysteps, and steps.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Skip the first two lines and process the rest\n",
    "    data = []\n",
    "    cumulative_steps = 0\n",
    "\n",
    "    for line in lines[2:]:  # Skip the first two lines\n",
    "        parts = line.strip().split(',')\n",
    "        date = parts[0]  # First column is the date\n",
    "\n",
    "        # Sum columns 2 to 5 (indices 1 to 4 in zero-based indexing)\n",
    "        daysteps = sum(map(int, parts[1:5]))\n",
    "\n",
    "        # Calculate cumulative steps\n",
    "        cumulative_steps += daysteps\n",
    "\n",
    "        # Append to data\n",
    "        data.append([date, daysteps, cumulative_steps])\n",
    "\n",
    "    # Create and return DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"date\", \"daysteps\", \"steps\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b1095",
   "metadata": {},
   "source": [
    "<h1>SAM segmentation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Anything Model initialization\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "mask_generator_1 = SAM2AutomaticMaskGenerator(sam2)\n",
    "mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    points_per_side=32,               # points_per_side: Optional[int] = 32,\n",
    "    points_per_batch=64,              # points_per_batch: int = 64,\n",
    "    pred_iou_thresh=0.8,              # pred_iou_thresh: float = 0.8,\n",
    "    stability_score_thresh=0.95,      # stability_score_thresh: float = 0.95,\n",
    "    stability_score_offset=1.0,       # stability_score_offset: float = 1.0,\n",
    "    crop_n_layers=0,                  # crop_n_layers: int = 0,\n",
    "    box_nms_thresh=0.7,               # box_nms_thresh: float = 0.7,\n",
    "    crop_n_points_downscale_factor=1, # crop_n_points_downscale_factor: int = 1,\n",
    "    min_mask_region_area=5.0,        # min_mask_region_area: int = 0,\n",
    "    use_m2m=False,                     # use_m2m: bool = False,\n",
    ")\n",
    "mask_generator=mask_generator_2\n",
    "\n",
    "image_files,images_list, masks_list = process_images_and_masks(image_path, masks_path, mask_generator_2)\n",
    "# plot_in_grid_with_titles(images_list,image_files)\n",
    "# #Load images\n",
    "# image_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.jpg')])\n",
    "# # image_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.jpg') and (f==base_image or f=='IMG_20240528_080644.jpg')])\n",
    "# images_list = [load_image(os.path.join(folder_path, file)) for file in image_files]\n",
    "# print(f'Loaded {len(images_list)} images from {folder_path}')\n",
    "\n",
    "# # Segment base image \n",
    "# base_image_ix = image_files.index(base_image)\n",
    "# masks_base_image = mask_generator.generate(images_list[base_image_ix])\n",
    "# masks_base_image = get_mask_by_image_name(base_image,image_files,masks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "851c52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_0 = expand_bounding_boxes_in_masks(images_list[base_image_ix], masks_base_image, matched_segments_bbox_expand)\n",
    "# align_and_plot_images(image_files,base_image_ix,images_list,masks_0, min_segment_area=min_segment_surface,max_segment_area=max_segment_surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108a53a",
   "metadata": {},
   "source": [
    "<h1>Warp images</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5de70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_image_ix = image_files.index(base_image)\n",
    "# masks_0 = expand_bounding_boxes_in_masks(images_list[base_image_ix], masks_base_image, matched_segments_bbox_expand)\n",
    "matched_results=[]\n",
    "matched_masks=[]\n",
    "images_to_warp = images_list[:len(images_list)]\n",
    "\n",
    "# 1. Match segments\n",
    "# for idx, image in enumerate(images_to_warp):\n",
    "#     results, masks = match_segments(images_list[base_image_ix], masks_0, image)\n",
    "#     matched_results.append(results)\n",
    "#     matched_masks.append(masks)\n",
    "\n",
    "# 2. Match and plot matched segments\n",
    "# for idx, image in enumerate(images_to_warp):\n",
    "#     results, masks = match_and_plot_segments(image_files[base_image_ix],image_files[idx], images_list[base_image_ix], masks_0, image)\n",
    "#     matched_results.append(results)\n",
    "#     # matched_top_masks.append(masks)\n",
    "#     matched_masks.append(masks)\n",
    "\n",
    "\n",
    "# 3. Match and warp and plot matched segments and warped image\n",
    "for idx, image in enumerate(images_to_warp):\n",
    "    print(f\"Working on {image_files[idx]}\")\n",
    "    if(image_files[idx].find('_reference.jpg') != -1):\n",
    "        base_image_ix = idx\n",
    "        masks_base_image = get_mask_by_image_name(image_files[base_image_ix],image_files,masks_list)\n",
    "        masks_0 = expand_bounding_boxes_in_masks(images_list[base_image_ix], masks_base_image, matched_segments_bbox_expand)\n",
    "        print(f'Using {image_files[base_image_ix]} as reference')\n",
    "    # results, masks, warped_image = match_warp_and_plot_segments_using_homography(image_files[base_image_ix],image_files[idx],  images_list[base_image_ix], masks_0, image)\n",
    "    results, masks, warped_image = match_warp_and_plot_segments_using_homography_with_rigid_transform_fallback(image_files[base_image_ix],image_files[idx],  images_list[base_image_ix], masks_0, image)\n",
    "    \n",
    "    warped_images_folder = homograpy_warped_images_folder\n",
    "    \n",
    "    # results, masks, warped_image = match_warp_and_plot_segments_using_translation(image_files[base_image_ix],image_files[idx],  images_list[base_image_ix], masks_0, image)\n",
    "    # warped_images_folder = translation_warped_images_folder\n",
    "    \n",
    "    # results, masks, warped_image = match_warp_and_plot_segments_using_translation_and_rotation(image_files[base_image_ix],image_files[idx],  images_list[base_image_ix], masks_0, image)\n",
    "    # warped_images_folder = translation_and_rotation_warped_images_folder\n",
    "    \n",
    "    if(warped_image is not None):\n",
    "        matched_results.append(results)\n",
    "        matched_masks.append(masks)\n",
    "        warped_image_bgr = cv2.cvtColor(warped_image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(f'{warped_images_folder}/{image_files[idx]}', warped_image_bgr)   \n",
    "    else:\n",
    "        print(f\"Cant't warp {image_files[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6a893",
   "metadata": {},
   "source": [
    "<h1>Crop - exif</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No EXIF datetime for the image\n",
      "No EXIF datetime for the image\n"
     ]
    }
   ],
   "source": [
    "warped_image_files = sorted(os.listdir(homograpy_warped_images_folder))\n",
    "os.makedirs(cropped_with_exif_images_folder,exist_ok=True)\n",
    "\n",
    "for file_name in warped_image_files:\n",
    "    warped_image_file_path = os.path.join(homograpy_warped_images_folder, file_name)\n",
    "    if file_name.lower().endswith(('.jpg')):\n",
    "        src_image_file_path = f'{src_image_path}/{file_name}'\n",
    "        warped_image = cv2.imread(warped_image_file_path)\n",
    "        if(warped_image is not None):\n",
    "            save_crop_exif(src_image_file_path,warped_image,crop_width,crop_height,cropped_with_exif_images_folder)\n",
    "        else:\n",
    "            print(f\"Cant't save_crop_exif {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f6776",
   "metadata": {},
   "source": [
    "<h1>Stop motion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e316a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import datetime\n",
    "import locale\n",
    "\n",
    "def create_stop_motion_movie_with_steps(input_folder, output_file, csv_file, frame_duration=2, transition_duration=1, fps=30, audio_file=None):\n",
    "    # Read CSV data\n",
    "    data = pd.read_csv(csv_file, parse_dates=['date'])\n",
    "\n",
    "    # Get all jpg files in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()  # Sort the files to ensure correct order\n",
    "    print(f\"files {image_files}\")\n",
    "\n",
    "    # Get the dimensions of the first image\n",
    "    first_image = cv2.imread(os.path.join(input_folder, image_files[0]))\n",
    "    height, width = first_image.shape[:2]\n",
    "\n",
    "    # Create a temporary video file\n",
    "    temp_output = 'temp_output.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize the line chart image\n",
    "    line_chart_file = 'line_chart.png'\n",
    "    full_data = data.copy()\n",
    "\n",
    "    # Loop through each image\n",
    "    for i in range(len(image_files)):\n",
    "        current_img = cv2.imread(os.path.join(input_folder, image_files[i]))\n",
    "        next_img = cv2.imread(os.path.join(input_folder, image_files[(i + 1) % len(image_files)]))\n",
    "        \n",
    "        # Resize images to ensure they have the same size\n",
    "        current_img = cv2.resize(current_img, (width, height))\n",
    "        next_img = cv2.resize(next_img, (width, height))\n",
    "\n",
    "        # Extract the date from the filename (format: IMG_yyyymmdd_hhmmss.jpg)\n",
    "        image_date_str = image_files[i][4:12]  # Extract yyyymmdd\n",
    "        image_date = datetime.strptime(image_date_str, '%Y%m%d')\n",
    "\n",
    "        # Filter the CSV data up to the current image date\n",
    "        filtered_data = data[data['date'] <= image_date]\n",
    "\n",
    "        # Update the line chart with the filtered data (this keeps the line chart updated with new data)\n",
    "        create_line_chart(filtered_data, full_data, line_chart_file)\n",
    "\n",
    "        # Superimpose the line chart on the current image (without blending it)\n",
    "        current_img_with_chart = overlay_line_chart(current_img, line_chart_file)\n",
    "\n",
    "        # Hold the current image (with the updated chart)\n",
    "        for _ in range(fps * frame_duration):\n",
    "            out.write(current_img_with_chart)\n",
    "\n",
    "        # Cross-fade the images, but without fading the line chart\n",
    "        for j in range(int(fps * transition_duration)):\n",
    "            alpha = j / (fps * transition_duration)\n",
    "            blended_img = cv2.addWeighted(current_img, 1 - alpha, next_img, alpha, 0)\n",
    "            # Overlay the line chart on the blended image (to keep it static during the transition)\n",
    "            blended_img_with_chart = overlay_line_chart(blended_img, line_chart_file)\n",
    "            out.write(blended_img_with_chart)\n",
    "\n",
    "    # Hold the last image if audio is longer than the video\n",
    "    last_img = current_img_with_chart  # The last image with the chart\n",
    "    video_duration = len(image_files) * (frame_duration + transition_duration)  # In seconds\n",
    "\n",
    "    # Get the duration of the MP3 audio file\n",
    "    if audio_file:\n",
    "        result = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', audio_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        audio_duration = float(result.stdout)\n",
    "        \n",
    "        if audio_duration > video_duration:\n",
    "            hold_duration = audio_duration - video_duration\n",
    "            print(f\"Holding the last frame for {hold_duration} seconds.\")\n",
    "            # Hold the last image for the remaining duration of the audio\n",
    "            for _ in range(int(hold_duration * fps)):\n",
    "                out.write(last_img)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    # FFmpeg command to add the MP3 audio to the video\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', temp_output,            # Input: stop motion movie\n",
    "        '-i', audio_file,             # Input: MP3 file\n",
    "        '-c:v', 'libx264',\n",
    "        '-preset', 'slow',\n",
    "        '-crf', '23',\n",
    "        '-vf', f'scale=-2:720',  # Scale to 720p, maintaining aspect ratio\n",
    "        '-movflags', '+faststart',\n",
    "        '-c:a', 'aac',\n",
    "        '-b:a', '128k',\n",
    "        '-ar', '44100',            # Resample audio to 44.1 kHz\n",
    "        '-shortest',               # Stops the video when the shorter stream ends (video or audio)\n",
    "        output_file\n",
    "    ]\n",
    "\n",
    "    # Run the FFmpeg command\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_output)\n",
    "\n",
    "    print(f\"Stop motion movie with audio created: {output_file}\")\n",
    "\n",
    "def overlay_line_chart(image, line_chart_file):\n",
    "    \"\"\"\n",
    "    Overlay the line chart in the bottom-right corner of the given image,\n",
    "    resizing it to 750px wide and 450px high.\n",
    "    \"\"\"\n",
    "    # Load the line chart as an image (with alpha channel for transparency)\n",
    "    line_chart_img = cv2.imread(line_chart_file, cv2.IMREAD_UNCHANGED)  # Load with transparency\n",
    "\n",
    "    # Resize the line chart to be 750px wide and 450px high\n",
    "    line_chart_img_resized = cv2.resize(line_chart_img, (750, 450))\n",
    "\n",
    "    # Get the dimensions of the resized chart\n",
    "    chart_height, chart_width = line_chart_img_resized.shape[:2]\n",
    "\n",
    "    # Define the region of interest (ROI) in the bottom-right corner\n",
    "    x_offset = image.shape[1] - chart_width - 10  # 10 pixels from the right\n",
    "    y_offset = image.shape[0] - chart_height - 10  # 10 pixels from the bottom\n",
    "\n",
    "    # If the line chart has an alpha channel, blend it with the image\n",
    "    if line_chart_img_resized.shape[2] == 4:\n",
    "        # Split the line chart into its color channels and alpha channel\n",
    "        b, g, r, alpha = cv2.split(line_chart_img_resized)\n",
    "\n",
    "        # Normalize the alpha channel to be between 0 and 1\n",
    "        alpha = alpha / 255.0\n",
    "\n",
    "        # Blend the line chart with the image\n",
    "        for c in range(0, 3):  # Iterate over the B, G, R channels\n",
    "            image[y_offset:y_offset+chart_height, x_offset:x_offset+chart_width, c] = (\n",
    "                alpha * line_chart_img_resized[:, :, c] +\n",
    "                (1 - alpha) * image[y_offset:y_offset+chart_height, x_offset:x_offset+chart_width, c]\n",
    "            )\n",
    "\n",
    "    return image\n",
    "\n",
    "def create_line_chart(filtered_data, full_data, output_file):\n",
    "    \"\"\"\n",
    "    Creates a static line chart from filtered data (up to a specific date) and saves it as an image.\n",
    "    The chart will have a light grey background with 70% opacity, display steps in 1K units with 1 decimal,\n",
    "    and annotate the last data point with the date in Dutch format (dd-mon).\n",
    "    \"\"\"\n",
    "    # Set locale to Dutch\n",
    "    locale.setlocale(locale.LC_TIME, 'nl_NL.UTF-8')  # For Dutch date formatting\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))  # Create a smaller chart (400x200px)\n",
    "\n",
    "    # Set light grey background for the figure with 70% opacity\n",
    "    fig.patch.set_facecolor(mcolors.to_rgba('lightgrey', 0.7))\n",
    "    ax.set_facecolor(mcolors.to_rgba('lightgrey', 0.7))\n",
    "\n",
    "    # Plot the filtered data\n",
    "    ax.plot(filtered_data['date'], filtered_data['steps'], lw=2)\n",
    "\n",
    "    # Calculate 5% margin for x-axis extension\n",
    "    date_range = full_data['date'].max() - full_data['date'].min()\n",
    "    margin = date_range * 0.10  # 5% of the date range\n",
    "\n",
    "    # Set the x-axis limits with the added margin\n",
    "    ax.set_xlim(full_data['date'].min(), full_data['date'].max() + margin)\n",
    "\n",
    "    # Set the y-axis limits\n",
    "    ax.set_ylim(full_data['steps'].min() * 0.9, full_data['steps'].max() * 1.1)\n",
    "\n",
    "    # Format y-axis to show steps in 1K units with 1 decimal\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.1f}K'))\n",
    "\n",
    "    # Annotate the last data point if available, also in 1K units\n",
    "    if not filtered_data.empty:\n",
    "        last_date = filtered_data['date'].iloc[-1]\n",
    "        last_value = filtered_data['steps'].iloc[-1] / 1000  # Convert to 1K units\n",
    "        last_date_str = last_date.strftime('%d %b')  # Format date as 'dd-mon' in Dutch\n",
    "\n",
    "        ax.annotate(f'{last_value:.1f}K\\n{last_date_str}',  # Display value in 1K units with 1 decimal and date\n",
    "                    xy=(last_date, last_value * 1000),  # Point at the last data point in original scale\n",
    "                    xytext=(5, 5),  # Slightly offset the text\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10,\n",
    "                    color='black')\n",
    "\n",
    "    # Remove axis lines and labels\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Save the chart as an image with a semi-transparent background\n",
    "    plt.savefig(output_file, dpi=100)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "619522ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['IMG_20240528_080644.jpg', 'IMG_20240529_073834.jpg', 'IMG_20240603_074710.jpg', 'IMG_20240605_073640.jpg', 'IMG_20240606_075254.jpg', 'IMG_20240607_073713.jpg', 'IMG_20240610_075152.jpg', 'IMG_20240611_074252.jpg', 'IMG_20240612_074106.jpg', 'IMG_20240613_073134.jpg', 'IMG_20240616_104257.jpg', 'IMG_20240617_082205.jpg', 'IMG_20240619_073804.jpg', 'IMG_20240621_083624.jpg', 'IMG_20240626_075046.jpg', 'IMG_20240627_074112.jpg', 'IMG_20240701_074430.jpg', 'IMG_20240703_073302.jpg', 'IMG_20240709_075414.jpg', 'IMG_20240715_080401.jpg', 'IMG_20240717_075449.jpg', 'IMG_20240722_073527.jpg', 'IMG_20240725_074323.jpg', 'IMG_20240726_074016.jpg', 'IMG_20240729_074638.jpg', 'IMG_20240802_073745.jpg', 'IMG_20240819_073730.jpg', 'IMG_20240820_074416.jpg', 'IMG_20240821_074526.jpg', 'IMG_20240823_074138.jpg', 'IMG_20240826_075635.jpg', 'IMG_20240828_080453.jpg', 'IMG_20240830_073307.jpg', 'IMG_20240903_080620.jpg', 'IMG_20240904_080904.jpg', 'IMG_20240906_075809.jpg', 'IMG_20240909_074851.jpg', 'IMG_20240913_075929.jpg', 'IMG_20240917_074348.jpg', 'IMG_20240918_075019.jpg', 'IMG_20240919_073731.jpg', 'IMG_20240920_074725.jpg', 'IMG_20240923_075129.jpg', 'IMG_20240924_080344.jpg', 'IMG_20240925_074705.jpg', 'IMG_20240926_073724.jpg', 'IMG_20240927_075346.jpg', 'IMG_20240930_082748.jpg', 'IMG_20241001_081025.jpg', 'IMG_20241002_075801.jpg', 'IMG_20241003_073739.jpg', 'IMG_20241004_075731.jpg', 'IMG_20241007_080041.jpg', 'IMG_20241008_080641.jpg', 'IMG_20241009_074244.jpg', 'IMG_20241014_075319.jpg', 'IMG_20241015_074834.jpg', 'IMG_20241016_075344.jpg', 'IMG_20241017_073918.jpg', 'IMG_20241018_074847.jpg', 'IMG_20241019_161538.jpg', 'IMG_20241021_122833.jpg', 'IMG_20241022_122630.jpg', 'IMG_20241022_122630_manual.jpg', 'IMG_20241023_122517.jpg', 'IMG_20241023_122517_manual.jpg', 'IMG_20241024_091334.jpg', 'IMG_20241024_091338.jpg', 'IMG_20241026_110249.jpg', 'IMG_20241026_152745.jpg', 'IMG_20241028_075511.jpg', 'IMG_20241029_124458.jpg', 'IMG_20241102_105516.jpg', 'IMG_20241104_122420.jpg', 'IMG_20241105_123037.jpg', 'IMG_20241106_080127.jpg', 'IMG_20241107_080917.jpg', 'IMG_20241108_081107.jpg', 'IMG_20241109_144127.jpg', 'IMG_20241112_134242.jpg', 'IMG_20241113_080058.jpg', 'IMG_20241114_123022.jpg', 'IMG_20241115_130713.jpg', 'IMG_20241118_122947.jpg', 'IMG_20241120_123340.jpg', 'IMG_20241121_080843.jpg', 'IMG_20241122_081953.jpg', 'IMG_20241124_081743.jpg', 'IMG_20241126_123524.jpg', 'IMG_20241128_080658.jpg', 'IMG_20241129_125911.jpg', 'IMG_20241130_140106.jpg', 'IMG_20241202_123625.jpg', 'IMG_20241202_123639.jpg', 'IMG_20241202_123641.jpg', 'IMG_20241202_123644.jpg', 'IMG_20241202_123647.jpg', 'IMG_20241203_123414.jpg', 'IMG_20241209_123038.jpg', 'IMG_20241212_161508.jpg', 'IMG_20241213_123518.jpg', 'IMG_20241216_123058.jpg', 'IMG_20241217_123247.jpg', 'IMG_20241218_124459.jpg', 'IMG_20241219_124846.jpg', 'IMG_20241220_123559.jpg']\n",
      "Holding the last frame for 3.709795999999983 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'temp_output.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf60.3.100\n",
      "  Duration: 00:04:53.43, start: 0.000000, bitrate: 23152 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 3000x1800 [SAR 1:1 DAR 5:3], 23151 kb/s, 30 fps, 30 tbr, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        handler_name    : VideoHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Input #1, mp3, from 'one_fine_day.mp3':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.20.100\n",
      "  Duration: 00:04:55.21, start: 0.025057, bitrate: 320 kb/s\n",
      "  Stream #1:0: Audio: mp3 (mp3float), 44100 Hz, stereo, fltp, 320 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavf\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "  Stream #1:0 -> #0:1 (mp3 (mp3float) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x12e618b00] using SAR=1/1\n",
      "[libx264 @ 0x12e618b00] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0x12e618b00] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x12e618b00] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=5 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=8 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=2 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=3 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=50 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/Users/peter/playground/parkbridge/auto_parkbridge.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf61.7.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 1200x720 [SAR 1:1 DAR 5:3], q=2-31, 30 fps, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        handler_name    : VideoHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.19.100 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.100 aac\n",
      "frame= 8692 fps=157 q=29.0 size=   76544KiB time=00:04:52.33 bitrate=2144.9kbits/s speed=5.28x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop motion movie with audio created: /Users/peter/playground/parkbridge/auto_parkbridge.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mp4 @ 0x12e614ef0] Starting second pass: moving the moov atom to the beginning of the file\n",
      "[out#0/mp4 @ 0x12e614e30] video:72355KiB audio:4621KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.389067%\n",
      "frame= 8803 fps=158 q=-1.0 Lsize=   77276KiB time=00:04:53.36 bitrate=2157.9kbits/s speed=5.27x    \n",
      "[libx264 @ 0x12e618b00] frame I:36    Avg QP:16.33  size:243420\n",
      "[libx264 @ 0x12e618b00] frame P:3616  Avg QP:20.35  size: 17901\n",
      "[libx264 @ 0x12e618b00] frame B:5151  Avg QP:21.66  size:   116\n",
      "[libx264 @ 0x12e618b00] consecutive B-frames: 20.5%  4.0%  1.7% 73.8%\n",
      "[libx264 @ 0x12e618b00] mb I  I16..4:  5.0% 54.5% 40.4%\n",
      "[libx264 @ 0x12e618b00] mb P  I16..4:  2.8%  1.8%  0.1%  P16..4: 40.5%  3.8%  6.5%  0.0%  0.0%    skip:44.5%\n",
      "[libx264 @ 0x12e618b00] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  3.3%  0.0%  0.0%  direct: 0.1%  skip:96.6%  L0:10.2% L1:88.6% BI: 1.1%\n",
      "[libx264 @ 0x12e618b00] 8x8 transform intra:41.3% inter:66.4%\n",
      "[libx264 @ 0x12e618b00] direct mvs  spatial:99.8% temporal:0.2%\n",
      "[libx264 @ 0x12e618b00] coded y,uvDC,uvAC intra: 27.6% 34.9% 15.4% inter: 11.8% 12.0% 1.1%\n",
      "[libx264 @ 0x12e618b00] i16 v,h,dc,p: 16% 39% 15% 30%\n",
      "[libx264 @ 0x12e618b00] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  5% 11% 33%  6% 10%  6% 11%  6% 13%\n",
      "[libx264 @ 0x12e618b00] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu:  7% 10% 13%  9% 12%  9% 12% 10% 17%\n",
      "[libx264 @ 0x12e618b00] i8c dc,h,v,p: 57% 30%  9%  4%\n",
      "[libx264 @ 0x12e618b00] Weighted P-Frames: Y:13.1% UV:9.7%\n",
      "[libx264 @ 0x12e618b00] ref P L0: 69.6% 22.8%  6.8%  0.5%  0.2%  0.1%  0.0%\n",
      "[libx264 @ 0x12e618b00] ref B L0: 91.6%  7.1%  1.0%  0.3%\n",
      "[libx264 @ 0x12e618b00] ref B L1: 95.9%  4.1%\n",
      "[libx264 @ 0x12e618b00] kb/s:2019.97\n",
      "[aac @ 0x12f80bf00] Qavg: 616.861\n"
     ]
    }
   ],
   "source": [
    "input_folder = cropped_with_exif_images_folder\n",
    "output_file = stop_motion_result_path\n",
    "create_stop_motion_movie_with_steps(input_folder, output_file, \"steps.csv\", transition_duration=0.75, audio_file='one_fine_day.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53009a1f",
   "metadata": {},
   "source": [
    "Show all the masks overlayed on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in images_list:\n",
    "    plot_overlayed(i,masks_0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
